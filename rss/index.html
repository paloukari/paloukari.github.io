<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[HAVE DATA - WILL TRAIN]]></title><description><![CDATA[The adventures of a Data Scientist who travels the Web working as a mercenary for people who hire him to solve their problems, but he provides his services for free to poor people who need his help]]></description><link>https://havedatawilltrain.com//</link><image><url>https://havedatawilltrain.com//favicon.png</url><title>HAVE DATA - WILL TRAIN</title><link>https://havedatawilltrain.com//</link></image><generator>Ghost 2.31</generator><lastBuildDate>Thu, 10 Oct 2019 14:51:07 GMT</lastBuildDate><atom:link href="https://havedatawilltrain.com//rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Three Threads to Perdido]]></title><description><![CDATA[This is the first part of a blog series that explores the topic of "High Throughput Object Detection" on the Edge.]]></description><link>https://havedatawilltrain.com//three-threads-to-perdido/</link><guid isPermaLink="false">5d321251435c041126c989a3</guid><category><![CDATA[TensorFlow]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[TensorRT]]></category><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Jetson TX2]]></category><category><![CDATA[Docker]]></category><category><![CDATA[ARM]]></category><category><![CDATA[Containers]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Fri, 04 Oct 2019 18:56:00 GMT</pubDate><media:content url="https://havedatawilltrain.com//content/images/2019/10/cowboys_twitter.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com//content/images/2019/10/cowboys_twitter.jpg" alt="Three Threads to Perdido"><p>Welcome stranger, I've been expecting you.</p><p>I know what brought you here, it's despair. I know what is out there,  I've seen it, and it's not pretty. Bad designs, broken code samples, no container definitions, missing dependency libraries, poor performance, you name it. </p><p>I was recently exploring ways to do<strong> real time object detection </strong>on my <strong>Nvidia Jetson TX2. </strong>The <em>real time </em>term here simply means, low latency and high throughput. It's a very loosely defined term, but it's used here in contrast to the store-and-process pattern, where storage is used as an interim stage.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="high-performance-objection-detection-on-a-jetson-tx2">High Performance Objection Detection on a Jetson TX2</h2><h3 id="starting-simple">Starting simple</h3><p>We'll explore a simple program that detects human faces using the camera input and renders the camera input with the bounding boxes. This one is based on the<strong> <a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html">Haar Cascades</a></strong><a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html"> </a>and is one of the simplest ways to get started with Object Detection on the Edge. There is no Jetson platform dependency for this code, only on <strong>OpenCV</strong>.</p><blockquote><em>I'm using a remote development setup to do all of my coding that uses containers. This way you can experiment with all the code samples yourself without having to setup any runtime dependencies on your device.</em><br><br><a href="https://havedatawilltrain.com//got-nano-will-code/"><strong>Here </strong></a>is how I setup my device and my <a href="https://havedatawilltrain.com//drama-of-entropy/"><strong>remote development environment</strong></a> with VSCode.</blockquote><p>Start by cloning the <a href="https://github.com/paloukari/jetson-detectors">example code</a>. After cloning, you need to build and run the container that we'll be using to run our code.</p><!--kg-card-begin: markdown--><h3 id="clonetheexamplerepo">Clone the example repo</h3>
<pre><code>https://github.com/paloukari/jetson-detectors
cd jetson-detectors
</code></pre>
<h3 id="tobuildandrunthedevelopmentcontainer">To build and run the development container</h3>
<pre><code>sudo docker build . -f ./docker/Dockerfile.cpu -t object-detection-cpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v &quot;$PWD&quot;:/src -p 32001:22 object-detection-cpu
</code></pre>
<!--kg-card-end: markdown--><p>The <code>--privileged</code> is required for accessing all the devices. Alternatively you can use the <code>--device /dev/video0</code>. </p><p><a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">Here's the code we'll be running.</a> Simple open the <code>cpudetector.py</code> file in VSCode and hit F5 or just run: <code>python3 src/cpudetector.py</code>. In both cases you'll need to setup the X forwarding. See the <a href="https://havedatawilltrain.com//got-nano-will-code/">Step 2: X forwarding</a> on how to do this.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/10/cpu-2.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~23 FPS with OpenCV</figcaption></figure><!--kg-card-end: image--><p><strong>We get about 23 FPS</strong>. Use the <code>tegrastats</code> to see what's happening in the GPU:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/10/image.png" class="kg-image" alt="Three Threads to Perdido"></figure><!--kg-card-end: image--><p>We're interested in the <code>GR3D_FREQ</code> values. It's clear that this code runs only on the device CPUs with more than 75% utilization per core, and with <strong>0% GPU utilization</strong>.</p><h3 id="next-up-we-use-go-deep">Next up, we use go Deep</h3><p>Haar Cascades is good, but how about detecting more things at once? In this case, we need to use Deep Neural Networks. We will need to use another container from now on to run the following code. </p><h3 id="to-build-and-run-the-gpu-accelerated-container">To build and run the GPU accelerated container</h3><!--kg-card-begin: code--><pre><code>sudo docker build . -f ./docker/Dockerfile.gpu -t object-detection-gpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v "$PWD":/src -p 32001:22 object-detection-gpu
</code></pre><!--kg-card-end: code--><blockquote>WARNING: This build takes a few hours to complete on a TX2. The main reason is because we build the Protobuf library to increase to models loading performance. To reduce this the build time, you can <a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson#building-jetson-containers-on-an-x86-workstation-using-qemu">build the same container on a X64 workstation</a>.</blockquote><p>In the first attempt, we'll be using the official <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">TensorFlow pre-trained networks</a>. The code we'll be running is <a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">here</a>.</p><p>When you run <code>python3 src/gpudetector.py --model-name ssd_inception_v2_coco</code> , the code will try to download the specified model inside the <code>/models</code> folder, and start the object detection in a very similar fashion as we did before. The <code>--model-name</code> default value is <code>ssd_inception_v2_coco</code>, so you can omit it.</p><p>This model has been trained to detect 90 classes (you can see the details in the downloaded <code>pipeline.config</code> file). Our performance plummeted to <strong>~8 FPS.</strong></p><p>Run <code>python3 src/gpudetector.py</code></p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/10/gpu.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~8 FPS with a the TensorFlow SSD Inception V2 COCO model</figcaption></figure><!--kg-card-end: image--><p>What happened? We've started running the inference in the GPU which for a single inference round trip now takes more time. Also, we move now a lot of data from the camera to RAM and from there to the GPU. This has an obvious performance penalty. </p><p>What we can do is start with optimizing the inference. We'll use the TensorRT optimization to speedup the inference. Run the same file as before, but now with the <code>--trt-optimize</code> flag. This flag will convert the specified TensorFlow mode to a TensorRT and save if to a local file for the next time.</p><p>Run <code>python3 gpudetector.py --trt-optimize</code>:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/10/ezgif-5-2f354c50d6b6.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~15 FPS with TensorRT optimization&nbsp;</figcaption></figure><!--kg-card-end: image--><p>Better, but still far from perfect. The way we can tell is by looking at the GPU utilization in the background, it drops periodically to 0%. This happens because the code is being executed sequentially. In other words, for each frame we have to wait to get the bits from the camera, create an in memory copy, push it to the GPU, perform the inference, and render the original frame with the scoring results.</p><p>We can break down this sequential execution to an asynchronous parallel version. We can have a dedicated thread for reading the data from the camera, one for running inference in the GPU and one for rendering the results.</p><p>To test this version, run  <code>python3 gpudetectorasync.py --trt-optimize</code></p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/10/gpurtasync.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~40 FPS with async TensorRT</figcaption></figure><!--kg-card-end: image--><p>By parallelizing expensive calculations, we've achieved a better performance compared to the OpenCV first example. The trade off now is that we've introduced a slight delay between the current frame and the corresponding inference result. To be more precise here, because the inference now is running on an independent thread, the observed FPS do not match with the number of inferences per second.</p><p><strong>What we have achieved:</strong> <u>We've explored different ways of improving the performance of the typical pedagogic object detection while-loop.</u></p><p>In a future post we'll explore how we can improve even more our detector, by using the <a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a>.</p>]]></content:encoded></item><item><title><![CDATA[Drama of entropy]]></title><description><![CDATA[I used to mess up my development environment over time by installing all the different experimentation dependencies. To package your app and avoid polluting your device OS with various dependencies, you can use Docker containers as development environments.]]></description><link>https://havedatawilltrain.com//drama-of-entropy/</link><guid isPermaLink="false">5d8aad58bea92c00581d20fc</guid><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Jetson TX2]]></category><category><![CDATA[Remote Debugging]]></category><category><![CDATA[VSCode]]></category><category><![CDATA[ARM]]></category><category><![CDATA[Docker]]></category><category><![CDATA[Containers]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Mon, 30 Sep 2019 20:17:07 GMT</pubDate><media:content url="https://havedatawilltrain.com//content/images/2019/09/Electrical-poles.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com//content/images/2019/09/Electrical-poles.jpg" alt="Drama of entropy"><p>I used to mess up my development environment over time by installing all the different experimentation dependencies. This is why I decided to switch to Docker containers.</p><p>The <strong>Jetson Nano </strong>default image comes with Docker runtime pre-installed. To package your app and avoid polluting your device OS with various dependencies, you can use Docker containers as development environments.</p><p>In fact, using a container as a development environment is a very similar to using directly a Jetson device for remote development. <a href="https://havedatawilltrain.com//got-nano-will-code/">Here's my post</a> on how to do the latter.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h2 id="remote-debugging-inside-a-container-running-on-a-jetson-device">Remote Debugging inside a Container running on a Jetson Device</h2><p></p><p>The <strong><a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson">NVIDIA Container Runtime on Jetson</a> </strong>repo<strong> </strong>has great information on how to use GPU accelerated containers in both the Jetson family, and an X64 workstation.</p><p>Once you choose what's your base container, you'll need to setup a few more things. </p><p>You can either clone <strong><a href="https://github.com/paloukari/jetson-detectors">this repo</a></strong> that contains all the below code, or start from scratch. The assumed file structure of all the following code is:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/10/image-2.png" class="kg-image" alt="Drama of entropy"></figure><!--kg-card-end: image--><blockquote><strong>/keys: </strong>Here's where I keep my public ssh key.<br><strong>/.vscode:</strong> Here's where I keep my debugging configuration.<br><strong>/docker: </strong>Inside the <strong>docker </strong>folder I put the container definitions.<br><strong>/src: </strong>Here's the application source code.</blockquote><h3 id="-keys">/keys</h3><p>You will need to setup a passwordless ssh between your host's VSCode and the container. To do this, you'll need to copy your ssh public key in the container's <code>authorized_keys</code> file. To get your public ssh key, run:<code>cat ~/.ssh/id_rsa.pub</code>. Copy the key value in the <code>id_rsa.pub</code> key file that's inside the /keys folder.</p><blockquote>My host is a Windows 10 machine and I generally make sure that my Windows and WSL keys are the same. To do this, I've copied my keys from <code>~/.ssh/</code> to <code>/mnt/c/users/[YOUR USERNAME]/.ssh</code>/. This way I get a convenience security symmetry.</blockquote><h3 id="-vscode">/.vscode</h3><p>Here's the my <code>launch.json</code> and how to setup an X Server for forwarding any GUI running on the container to your host (<a href="https://havedatawilltrain.com//got-nano-will-code/">here's</a> more information how to set this up)</p><!--kg-card-begin: markdown--><pre><code>{    
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;console&quot;: &quot;integratedTerminal&quot;, 
            &quot;env&quot;:
            {
            &quot;DISPLAY&quot;: &quot;10.135.62.79:0.0&quot; 
            }
        }
    ]
}
</code></pre>
<!--kg-card-end: markdown--><blockquote>Replace the IP with your host's IP. I'm using the <strong>X410 X Server</strong> on Windows 10.</blockquote><h3 id="-docker">/docker</h3><p>In this example, I've started from a bare minimum, the<strong> l4t OS</strong>. This dockerfile is self explanatory. </p><!--kg-card-begin: markdown--><pre><code>FROM nvcr.io/nvidia/l4t-base:r32.2

ENV DEBIAN_FRONTEND=noninteractive

# Install Python3, Git and OpenCV
RUN apt-get update &amp;&amp; apt-get --yes install openssh-server python3-dev python3-pip python3-opencv git
RUN pip3 install --upgrade pip

RUN pip3 install click

ENV LC_ALL C.UTF-8
ENV LANG C.UTF-8

# Set the WORKDIR
WORKDIR /src

ENTRYPOINT service ssh restart &amp;&amp; bash

# Install the ssh public key - Remove this in a production deployment
COPY ./keys/id_rsa.pub /tmp/tmp.pub
RUN mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh &amp;&amp; cat /tmp/tmp.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 ~/.ssh/authorized_keys &amp;&amp; rm -f /tmp/tmp.pub

</code></pre>
<!--kg-card-end: markdown--><h3 id="-src">/src</h3><p>This is where I put all the application code. You can test that the X forwarding works by opening running the <code>xtest.py</code>. You'll have to install the <code>eog</code> application in the container first by running <code>apt-get install eog</code> in a new VSCode terminal.</p><p>Here's the <code>xtest.py</code> code:</p><!--kg-card-begin: markdown--><pre><code>import subprocess
subprocess.run([&quot;eog&quot;])
</code></pre>
<!--kg-card-end: markdown--><h2 id="wiring-everything-up">Wiring everything up</h2><h3 id="on-your-jetson-device-run-">On your Jetson device, run:</h3><!--kg-card-begin: markdown--><pre><code># Clone the example repo
git clone https://github.com/paloukari/jetson-detectors
cd jetson-remote-development

# Build the dev container
sudo docker build . -f ./docker/Dockerfile.cpu -t object-detection-cpu

# Run the container
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v &quot;$PWD&quot;:/src -p 32001:22 object-detection-cpu
</code></pre>
<!--kg-card-end: markdown--><p>In your host's VSCode, add this information in the Remote-SSH configuration file:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/09/image-6.png" class="kg-image" alt="Drama of entropy"></figure><!--kg-card-end: image--><!--kg-card-begin: markdown--><pre><code>Host Nano
    User spyros
    HostName spyros-nano2
    IdentityFile ~/.ssh/id_rsa

Host NanoContainer
    User root
    HostName spyros-nano2
    IdentityFile ~/.ssh/id_rsa
    Port 32001

</code></pre>
<!--kg-card-end: markdown--><p>Now, you should be able to connect to the NanoContainer Remote SSH host. Last step is to install the Python VSCode extension on the remote host. </p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/09/image-8.png" class="kg-image" alt="Drama of entropy"><figcaption>The VSCode extensions need to be installed in both machines in a Remote-SSH session</figcaption></figure><!--kg-card-end: image--><p>Open the <code>xtest.py</code> and fit F5. </p><blockquote>You need to install the eog app in the container.</blockquote><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/10/ezgif-5-44d36401c343.gif" class="kg-image" alt="Drama of entropy"></figure><!--kg-card-end: image--><p><strong>What we achieved: </strong><u>We can now do remote development from a Windows host to a container running on a Jetson Nano, using VSCode.</u></p>]]></content:encoded></item><item><title><![CDATA[Got Nano, Wanna Code]]></title><description><![CDATA[Learn how to setup a remote development environment using VSCode from a Windows machine on a ARM device]]></description><link>https://havedatawilltrain.com//got-nano-will-code/</link><guid isPermaLink="false">5d7bf22d3694ea005ad2a5f8</guid><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Remote Debugging]]></category><category><![CDATA[VSCode]]></category><category><![CDATA[ARM]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Wed, 25 Sep 2019 20:39:41 GMT</pubDate><media:content url="https://havedatawilltrain.com//content/images/2019/09/Jetson-Nano_3QTR-Front_Left_trimmed-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com//content/images/2019/09/Jetson-Nano_3QTR-Front_Left_trimmed-1.jpg" alt="Got Nano, Wanna Code"><p>So, you just got your Jetson Nano and you're wondering how to get started? In this blog post you'll learn how to setup a remote development environment from your Windows 10 machine to a Jetson device.</p><h2 id="device-setup">Device Setup</h2><p>First, let's start with the device initial setup. To prepare my device, I followed the official <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">getting started guide </a>from Nvidia. I used a <a href="https://www.amazon.com/SanDisk-128GB-Extreme-microSD-Adapter/dp/B07FCMKK5X/ref=asc_df_B07FCMKK5X/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=309776868400&amp;hvpos=1o1&amp;hvnetw=g&amp;hvrand=8725959158767136122&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1027744&amp;hvtargid=pla-588455240877&amp;psc=1">128 GB microSD</a> and a <a href="https://www.amazon.com/5V-4000mA-switching-power-supply/dp/B01LY5TG5Y">power adapter</a> to power up my device, along with the <a href="https://www.jetsonhacks.com/2019/04/10/jetson-nano-use-more-power/">required jumper</a>. I connected a Dell P2415Q monitor using an HDMI cable and my USB receiver for my Logitech Triathlon keyboard and mouse. My development laptop is a Windows 10 machine.</p><h2 id="desktop-sharing">Desktop Sharing </h2><p>The desktop sharing app is broken. Follow <a href="https://blog.hackster.io/getting-started-with-the-nvidia-jetson-nano-developer-kit-43aa7c298797">this</a> article to fix it. You can also find instructions there on how to use Microsoft's Remote Desktop.</p><h2 id="development-environment">Development Environment</h2><p>The latest trend of development experience in the software development industry is developing using <strong>CLIs </strong>and <strong>code editors</strong>. I generally enjoy having all moving parts together when I'm coding, or at least in proximity.</p><p><strong>Visual Studio Code</strong> succeeded in combining together a cross-platform code editor, community driven plugins and decent UI/UX for development. But what I really like with Visual Studio Code is the recent <strong><a href="https://code.visualstudio.com/docs/remote/remote-overview">Remote Development</a> </strong>capability that allows you to keep a local UI/UX experience, while working on a remote host or container.</p><p>The other thing I like having in my development environment is <strong>idempotency</strong>: not having to deal with the dependencies of the host. <strong>Containers </strong>allow me to have multiple dependency configurations on the same host, and moreover, to share code between different hosts. For this reason, in every code sample I present in this blog, I make sure to include the corresponding container.</p><h3 id="step-1-remote-development-on-a-jetson-nano">Step 1: Remote Development on a Jetson Nano</h3><p>Because the ARM64 architecture is not officially supported yet, to install<strong> Visual Studio Code</strong> on your <strong>Jetson Nano</strong>, for now you'll have to use the <a href="https://code.headmelted.com/">community binaries</a>.</p><p><strong>Installing Visual Studio Code on a Jetson Nano</strong></p><!--kg-card-begin: code--><pre><code class="language-Bash"># Start an elevated session
sudo -s

# Install the community repo key
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0CC3FD642696BFC8

# Run the installation script
. &lt;( wget -O - https://code.headmelted.com/installers/apt.sh )
</code></pre><!--kg-card-end: code--><p>If all goes well, you should get this output:</p><!--kg-card-begin: code--><pre><code class="language-Bash">Installation complete!

You can start code at any time by calling "code-oss" within a terminal.

A shortcut should also now be available in your desktop menus (depending on your distribution).</code></pre><!--kg-card-end: code--><p>Next, you'll need to install the <strong><a href="https://code.visualstudio.com/insiders/">Visual Studio Code Insiders</a> on your host </strong>and install the <strong><a href="https://code.visualstudio.com/docs/remote/remote-overview">Remote Development</a> </strong>extension.</p><p>As a final step, you'll need to setup a passwordless SSH between your host and the Nano. To do this, you'll need to create an SSH public-private key pair and configure your device to trust your public key.</p><p>To setup your local SSH key in Windows run:</p><!--kg-card-begin: code--><pre><code class="language-Powershell">Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0
ssh-keygen -t rsa -b 4096 
</code></pre><!--kg-card-end: code--><p>The keys will be created here: %USERPROFILE%\.ssh</p><p>To copy your public key to the device:</p><!--kg-card-begin: code--><pre><code class="language-CMD">SET REMOTEHOST=user@device
scp %USERPROFILE%\.ssh\id_rsa.pub %REMOTEHOST%:~/tmp.pub
ssh %REMOTEHOST% "mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh &amp;&amp; cat ~/tmp.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 ~/.ssh/authorized_keys &amp;&amp; rm -f ~/tmp.pub"</code></pre><!--kg-card-end: code--><!--kg-card-begin: markdown--><blockquote>
<p>You'll need to replace the value <strong>user@device</strong></p>
</blockquote>
<!--kg-card-end: markdown--><p>In you host VSCode, hit F1 and type <em>Remote-SSH: Open Configuration File..</em></p><p>Add your configuration:</p><!--kg-card-begin: code--><pre><code class="language-Remote-SSH config">Host Nano
    User user
    HostName device
    IdentityFile ~/.ssh/id_rsa</code></pre><!--kg-card-end: code--><!--kg-card-begin: markdown--><blockquote>
<p>Replace the values <strong>user</strong> and <strong>device</strong></p>
</blockquote>
<!--kg-card-end: markdown--><p>Connect to your Nano clicking the low left corner green icon:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/09/image-1.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><!--kg-card-end: image--><p>Congrats! Once connected, you can open a remote folder and open remote terminals directly in VSCode.</p><h3 id="step-2-x-forwarding">Step 2: X forwarding</h3><p>What about applications that have a GUI? No worries, you can setup an X forwarding from Nano to your host. To do this, I'm using the <a href="https://token2shell.com/x410/"><strong>X410 server</strong></a> for Windows. After installing and running the server, make sure you allow public access.</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/09/image-2.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><!--kg-card-end: image--><p>All is left to do is to configure the X Forwarding on your Nano device.</p><p>Open a terminal in VSCode (<strong>Ctrl+Shift+`</strong> on Windows) and run:</p><!--kg-card-begin: code--><pre><code class="language-Remote-SSH config">export DISPLAY=10.135.62.79:0.0</code></pre><!--kg-card-end: code--><!--kg-card-begin: markdown--><blockquote>
<p>Make sure you replace the above IP with your host IP</p>
</blockquote>
<!--kg-card-end: markdown--><p> To verify everything works, from the terminal run:</p><!--kg-card-begin: code--><pre><code class="language-Remote-SSH config">eog</code></pre><!--kg-card-end: code--><p>If all goes well, you should see this window:</p><!--kg-card-begin: image--><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com//content/images/2019/09/image-3.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><!--kg-card-end: image--><p>To automatically set the remote variable when debugging your app, you can modify your<strong> launch.json </strong>and set the variable there:</p><!--kg-card-begin: code--><pre><code class="language-launch.json">{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal", 
            "env":
            {
            "DISPLAY": "10.135.62.79:0.0" 
            }
        }
    ]
}</code></pre><!--kg-card-end: code--><p>To test all this, create a new python file containing the following code:</p><!--kg-card-begin: code--><pre><code class="language-test.py">import subprocess
subprocess.run(["eog"])</code></pre><!--kg-card-end: code--><p>Now you can hit <strong>F5 </strong>and you'll get the same development experience, as with your local host, but<strong> running on a remote ARM machine!</strong></p><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com//content/images/2019/09/ezgif-5-44d36401c343.gif" class="kg-image" alt="Got Nano, Wanna Code"><figcaption>Starting a remote debugging session on Jetson Nano</figcaption></figure><!--kg-card-end: image--><p><strong>What we have achieved: </strong><u>We can now do remote development from a Windows host to a Jetson Nano, using VSCode.</u></p><p>Next, we'll setup the same remote environment, <a href="https://havedatawilltrain.com//drama-of-entropy/">but on a Docker container running on the Jetson device</a>.</p>]]></content:encoded></item></channel></rss>