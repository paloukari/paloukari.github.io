<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[HAVE DATA - WILL TRAIN]]></title><description><![CDATA[The adventures of a Data Scientist who travels the Web working as a mercenary for people who hire him to solve their problems, but he provides his services for free to poor people who need his help]]></description><link>https://havedatawilltrain.com/</link><image><url>https://havedatawilltrain.com/favicon.png</url><title>HAVE DATA - WILL TRAIN</title><link>https://havedatawilltrain.com/</link></image><generator>Ghost 3.12</generator><lastBuildDate>Mon, 06 Apr 2020 18:59:40 GMT</lastBuildDate><atom:link href="https://havedatawilltrain.com/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[The Waiting Loop]]></title><description><![CDATA[This is the third post of a mini-series that aims to address the development experience and production readiness challenges in the space of IoT Edge, primarily under the lens of Azure IoT Edge. This post deals with the topic of the telemetry pump.]]></description><link>https://havedatawilltrain.com/the-waiting-loop/</link><guid isPermaLink="false">5e860b6b188f260074a3084e</guid><category><![CDATA[Azure IoT Edge]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Sat, 04 Apr 2020 00:48:28 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2020/04/penguins-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2020/04/penguins-1.jpg" alt="The Waiting Loop"><p>In IoT, a very common coding pattern is having an endless loop that does some periodic work and emits telemetry based on the work result. In the example we saw in the <a href="https://havedatawilltrain.com/restarted-by-request/">previous post</a>, we used the <code>EmitTelemetryMessagesAsync</code> to simulate a temperature sensor, but in real life, this function would read the value of an actual hardware temperature sensor and send out the temperature telemetry event. From now on, we will call this pattern <strong>telemetry pump.</strong></p><p>The telemetry pump pattern is very old. In fact, every Windows application has a similar pump, but for UI events instead of telemetry events. This pump is the dispatching mechanism of the OS events to your application. Similarly in the IoT context, a telemetry pump acts as the dispatching interface between the hardware sensors and the cloud.</p><h2 id="publisher-subscriber">Publisher/Subscriber </h2><p>In an ideal world, a publisher/subscriber pattern that is based on hardware interrupts would be a more elegant approach. Nevertheless, <strong>the hardware world is full of uncertainty: </strong>originating in Heisenberg's uncertainty principle, there is always some degree of noise when we try to measure any physical phenomenon. This means that consecutive measurement values will jitter (try measuring voltage with a high-accuracy voltmeter). For this reason, most of the times we're forced to use <strong>periodical polling </strong>instead of change-driven updates. And depending on the required data resolution, we can come up with the <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">required minimum frequency</a>.</p><h2 id="timers">Timers </h2><p>Many telemetry pumps designs exist, depending on the depth of the technology stack we're working on. The periodical nature of the sensor polling often inspires people to use <a href="https://docs.microsoft.com/en-us/windows/win32/sync/using-timer-queues">timers</a>. Although timers may seem conceptually a suitable and easy approach, timers usually get you in trouble. <u>There is no guarantee that the previous iteration has completed before the timer fires again</u>. Indeed, if for example we're emitting a telemetry message every second, assuming that every iteration of reading the sensor and sending the message takes roughly 300msec to complete, then the invocation callback is guaranteed to complete before the next period starts. </p><p>Graphically, this timeline looks like:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/image-20.png" class="kg-image" alt="The Waiting Loop"><figcaption>A timer-based periodic invocation timeline</figcaption></figure><p>But as we briefly touched on this in a <a href="https://havedatawilltrain.com/the-last-judgment/">previous post</a>, <strong>there is no guarantee of an IO operation duration. </strong>Let's say for example that because of a network glitch, the t+1 callback invocation takes more time to complete:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/image-8.png" class="kg-image" alt="The Waiting Loop"><figcaption>A timer-based periodic invocation timeline during a network glitch</figcaption></figure><p>The danger in this case is that at the cloud side we might end up with <strong>out-of-order messages </strong>that we'll then have to add special care to determine their actual order (we'll see next that we might have to do this either way). But most importantly, because we have parallel execution of the same code, we need to make sure that the code <strong>is thread safe, </strong>in other words, to ensure that we use <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.semaphore?view=netcore-3.1">semaphores</a> when we access shared (heap) memory.</p><p>An even more dangerous case leads to thread poll exhaustion. This can happen when instead of just a network glitch, we run into a network bottleneck that causes significant delay for all callback invocation completion:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/image-15.png" class="kg-image" alt="The Waiting Loop"><figcaption>A timer-based periodic invocation timeline during a network bottleneck</figcaption></figure><p> In this case, a catastrophic program failure is inevitable.</p><h2 id="task-timer">Task-timer</h2><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2020/04/daisyChain2.gif" class="kg-image" alt="The Waiting Loop"></figure><p>An alternative approach that mitigates these issues is to use to <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/chaining-tasks-by-using-continuation-tasks">chained tasks</a>. Although this approach might seem initially more complex, in reality is very straight forward. In C#, a <a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/task-based-asynchronous-programming">Task </a>can have a continuation Task, which simply is another task that starts right after the former completes. By chaining together multiple callback invocation tasks and with adding the necessary <code>Task.Delay</code> between them, we can have the same periodical behavior as before:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/image-19.png" class="kg-image" alt="The Waiting Loop"><figcaption>A task-timer periodic invocation timeline</figcaption></figure><p>Going back to the previous network glitch scenario, the chained tasks will never produce any thread-unsafe or out-of-order messages cases, but rather just a period drift in just a single iteration:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/image-18.png" class="kg-image" alt="The Waiting Loop"><figcaption>A task-timer periodic invocation timeline during a network glitch.</figcaption></figure><p> The downside with this approach is that we lose our polling periodicity. Note that the delay in Callback #2 has shifted all the subsequent callbacks.</p><p>A task-timer implementation, as described above, is the code snippet below:</p><pre><code class="language-csharp">using System;
using System.Threading;
using System.Threading.Tasks;

public class TaskTimer
{
    CancellationTokenSource cancellationTokenSource;
    TimeSpan timerPeriod;
    Action onElapsedCallback;
    bool continueOnError;

    public TaskTimer(Action onElapsedCallback,
        TimeSpan timerPeriod,
        CancellationTokenSource cancellationTokenSource,
        bool continueOnError = true)
    {
        this.cancellationTokenSource = cancellationTokenSource;
        this.timerPeriod = timerPeriod;
        this.onElapsedCallback = onElapsedCallback;
        this.continueOnError = continueOnError;
    }

    public void Start()
    {
        Task elapsedTask = null;
        elapsedTask = new Task((x) =&gt;
        {
            Elapsed(elapsedTask, cancellationTokenSource);
        }, cancellationTokenSource.Token);

        HandleError(elapsedTask);

        elapsedTask.Start();
    }

    private void Elapsed(Task task, object objParam)
    {
        var start = DateTime.Now;
        var cancellationTokenSource = (CancellationTokenSource)objParam;
        if (cancellationTokenSource.Token.IsCancellationRequested)
        {
            Console.WriteLine("TaskTimer: A cancellation has been requested.");
            return;
        }

        onElapsedCallback();

        var delay = timerPeriod - (DateTime.Now - start);
        if (delay.Ticks &gt; 0)
        {
            task = Task.Delay(delay);
        }
        HandleError(task.ContinueWith(Elapsed, cancellationTokenSource));
    }

    private void HandleError(Task task)
    {
        task.ContinueWith((e) =&gt;
        {
            Console.WriteLine(
                $"Exception when running timer callback: {e.Exception}");
            if (!continueOnError)
                cancellationTokenSource.Cancel();
            else
                task.ContinueWith(Elapsed, cancellationTokenSource);
        }, TaskContinuationOptions.OnlyOnFaulted);
    }
}</code></pre><p><u>A few of notes about this <code>TaskTimer</code>: </u></p><p>The endless task-chaining is achieved by creating a <code>Task</code> with execution entry point in the <code>Elapsed</code> method (our callback) and by passing a reference of this task in the <code>Task</code> entry point arguments:</p><pre><code class="language-csharp">Task elapsedTask = null;
elapsedTask = new Task((x) =&gt;
{
	Elapsed(elapsedTask, cancellationTokenSource);
}, cancellationTokenSource.Token);</code></pre><p>Next, inside this <code>Elapsed</code> method, we continue the argument task with the same method:</p><pre><code class="language-csharp">task.ContinueWith(Elapsed, cancellationTokenSource)</code></pre><p>Between each continuation task, a delay might be added, based on the duration of the last iteration and the configured period:</p><pre><code class="language-csharp">var delay = timerPeriod - (DateTime.Now - start);
if (delay.Ticks &gt; 0)
{
    task = Task.Delay(delay);
}</code></pre><p>This implementation will break the task-chaining if the passed <code>CancellationToken</code> signals a cancellation:</p><pre><code class="language-csharp">if (cancellationTokenSource.Token.IsCancellationRequested)
{
    Console.WriteLine("TaskTimer: A cancellation has been requested.");
    return;
}</code></pre><p>Finally, the task-timer will trigger a cancellation signal if an exception is thrown by the callback, and the <code>continueOnError</code> is set to <code>false</code>:</p><pre><code class="language-csharp">private void HandleError(Task task)
{
    task.ContinueWith((e) =&gt;
    {
		Console.WriteLine(
            $"Exception when running timer callback: {e.Exception}");
        if (!continueOnError)
	        cancellationTokenSource.Cancel();
        else
    	    task.ContinueWith(Elapsed, cancellationTokenSource);
    }, TaskContinuationOptions.OnlyOnFaulted);
}</code></pre><p>Let's see now how our sample temperature module evolves using the this <code>TaskTimer</code>:</p><pre><code class="language-csharp">using System;
using System.Runtime.Loader;
using System.Threading;
using System.Threading.Tasks;

namespace Example4
{
    class Program
    {
        // This is the program entry point
        static async Task Main(string[] args)
        {
            // Create the cancellation token source
            var cancellationTokenSource = new CancellationTokenSource();
            AssemblyLoadContext.Default.Unloading +=
                (cts) =&gt; cancellationTokenSource.Cancel();

            Console.CancelKeyPress +=
                (sender, cts) =&gt;
                {
                    Console.WriteLine("Ctrl+C detected.");
                    cts.Cancel = true;
                    cancellationTokenSource.Cancel();
                };

            // Register the Reset command callback 
            await RegisterCommandCallbackAsync("Reset",
                OnReset,
                cancellationTokenSource.Token);

            // Create and start the TaskTimer
            TaskTimer taskTimer = new TaskTimer(
                EmitTelemetryMessage,
                TimeSpan.FromSeconds(1), cancellationTokenSource);

            // A non-blocking call to start the task-timer
            taskTimer.Start();

            // Wait until the app unloads or gets cancelled
            await WhenCancelled(cancellationTokenSource.Token);

            // Let the other threads drain
            Console.WriteLine("Waiting for 2 seconds..");
            await Task.Delay(TimeSpan.FromSeconds(2));

            Console.WriteLine("Exiting..");
        }

        public static Task WhenCancelled(CancellationToken cancellationToken)
        {
            var taskCompletionSource = new TaskCompletionSource&lt;bool&gt;();
            cancellationToken.Register(
                s =&gt; ((TaskCompletionSource&lt;bool&gt;)s).SetResult(true),
                taskCompletionSource);
            return taskCompletionSource.Task;
        }
        private static async Task RegisterCommandCallbackAsync(string command,
            Action callback,
            CancellationToken cancellationToken)
        {
            // Perform the command registration
            // Code omitted
            return;
        }

        // A method exposed for RPC
        static void OnReset()
        {
            // Perform a temperature sensor reset
            // Code omitted
        }

        // Emit telemetry message
        static void EmitTelemetryMessage()
        {
            Console.WriteLine($"Sending telemetry message..");
        }
    }
}
</code></pre><p>Arguably, this version is more readable and at the same time, we've avoided all aforementioned pitfalls. Running this version now produces a similar output as before:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2020/04/task-timer.gif" class="kg-image" alt="The Waiting Loop"></figure><p>So far, we've been using the <code>Console.WriteLine</code> function to print messages and we've avoided reading any application configuration. In the next post, we'll examine the best approaches on these topics.</p>]]></content:encoded></item><item><title><![CDATA[Restarted by request]]></title><description><![CDATA[This is the second post of a mini-series that aims to address the development experience and production readiness challenges in the space of IoT Edge, primarily under the lens of Azure IoT Edge. This post deals with the topic of application restarting.]]></description><link>https://havedatawilltrain.com/restarted-by-request/</link><guid isPermaLink="false">5e86096c188f260074a3083d</guid><category><![CDATA[Azure IoT Edge]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Sat, 04 Apr 2020 00:03:31 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2020/04/apollo.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2020/04/apollo.jpg" alt="Restarted by request"><p>In the <a href="https://havedatawilltrain.com/the-last-judgment/">previous post</a> we briefly touched on holding the main method from restarting, and as a trick, we used the <code>Console.ReadLine()</code> blocking call. But as we postulated, it's good to avoid any thread-blocking function calls. Furthermore, the process termination and recycling is a more complicated story, especially for code that is running on the edge, perhaps on a remote device with no keyboard, mouse or monitor to interact with. A better strategy in these cases is to control the process termination and use it as a recovery mechanism from unexpected situations, commonly called <strong>fatal exceptions. </strong></p><h2 id="application-termination-and-recycling">Application termination and recycling</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/04/onAndOff.gif" class="kg-image" alt="Restarted by request"><figcaption>The best troubleshooting strategy</figcaption></figure><h3 id="recycling">Recycling</h3><p>The Azure IoT Edge hosting mechanism can be configured to restart a stopped module through the <a href="https://docs.microsoft.com/en-us/azure/iot-edge/iot-edge-runtime "><strong>restartPolicy</strong></a><strong> </strong>configuration in the application manifest. The main point here is that the hosting runtime assumes that the application might crash and exit, and allowing the application to restart is a very appealing crash recovery mechanism that we should definitely leverage.</p><p>The <strong>restartPolicy</strong> configuration dictates how the IoT Edge agent restarts a module. Possible values include:</p><ul><li><code>never</code> – The IoT Edge agent never restarts the module.</li><li><code>on-failure</code> - If the module crashes, the IoT Edge agent restarts it. If the module shuts down cleanly, the IoT Edge agent doesn't restart it.</li><li><code>on-unhealthy</code> - If the module crashes or is considered unhealthy, the IoT Edge agent restarts it.</li><li><code>always</code> - If the module crashes, is considered unhealthy, or shuts down in any way, the IoT Edge agent restarts it.</li></ul><p>The product has other ways to remotely stop and start any module in an ad-hoc manner, so setting the <strong>restartPolicy</strong> to <code>always</code> is, generally speaking, a good strategy.</p><h3 id="cancellation">Cancellation</h3><p>In the second code sample of the <a href="https://havedatawilltrain.com/the-last-judgment/">previous post</a>, we saw how to use the async/await pattern to implement non-blocking code parallelism. But in that approach, there was no way to signal the application to exit gracefully, and this can become particularly challenging in highly parallel code. The .NET Core runtime provides a parallel operation cancellation mechanism through the <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.cancellationtokensource?view=netcore-3.1"><strong>CancellationTokenSource</strong></a><strong>. </strong></p><blockquote>When a fatal exception issue occurs, a <u>graceful exit is always preferred</u> versus an application crash, because this allows debugging information to be written in the application logs for post-mortem analysis.</blockquote><p>Now, let's see how our code evolves with the usage of a CancellationTokenSource:</p><pre><code class="language-csharp">using System;
using System.Runtime.Loader;
using System.Threading;
using System.Threading.Tasks;

namespace Example3
{
    class Program
    {
        // This is the program entry point
        static async Task Main(string[] args)
        {
            // Create the cancellation token source
            var cancellationTokenSource = new CancellationTokenSource();
            AssemblyLoadContext.Default.Unloading += 
                (cts) =&gt; cancellationTokenSource.Cancel();
            
            Console.CancelKeyPress +=
                (sender, cts) =&gt;
                {
                    Console.WriteLine("Ctrl+C detected.");
                    cts.Cancel = true;
                    cancellationTokenSource.Cancel();
                };

            // Register the Reset command callback 
            await RegisterCommandCallbackAsync("Reset", 
                OnReset, 
                cancellationTokenSource.Token);

            // A non-blocking telemetry emission invocation
            await EmitTelemetryMessagesAsync(cancellationTokenSource.Token);

            // Wait until the app unloads or gets cancelled
            await WhenCancelled(cancellationTokenSource.Token);

            // Let the other threads drain
            Console.WriteLine("Waiting for 2 seconds..");
            await Task.Delay(TimeSpan.FromSeconds(2));

            Console.WriteLine("Exiting..");
        }

        public static Task WhenCancelled(CancellationToken cancellationToken)
        {
            var taskCompletionSource = new TaskCompletionSource&lt;bool&gt;();
            cancellationToken.Register(
                s =&gt; ((TaskCompletionSource&lt;bool&gt;)s).SetResult(true), 
                taskCompletionSource);
            return taskCompletionSource.Task;
        }
        private static async Task RegisterCommandCallbackAsync(string command,
            Action callback,
            CancellationToken cancellationToken)
        {
            // Perform the command registration
            // Code omitted
            return;
        }

        // A method exposed for RPC
        static void OnReset()
        {
            // Perform a temperature sensor reset
            // Code omitted
        }

        // Emit telemetry messages
        static async Task EmitTelemetryMessagesAsync(
            CancellationToken cancellationToken)
        {
            while(true)
            {
                if (cancellationToken.IsCancellationRequested)
                {
                    Console.WriteLine($"Exiting telemetry pump..");
                    break;
                }
                Console.WriteLine($"Sending telemetry message..");
                await Task.Delay(TimeSpan.FromSeconds(1));
            }
        }
    }
}</code></pre><p>The cancellation token source allows us to signal a cancellation by invoking the <code>Cancel()</code> method, as we decided to do when the <code>AssemblyLoadContext.Default.Unloading</code> fires, an event that generally means your application is already exiting, or when the <code><a href="https://docs.microsoft.com/en-us/dotnet/api/system.console.cancelkeypress?view=netcore-3.1 ">Console.CancelKeyPress</a></code> event fires (Ctrl+C), useful for testing the cancellation mechanism in our development environment. A requested cancellation can be detected by the <code>cancellationToken.IsCancellationRequested</code> property, and then make sure we gracefully terminate any running parallel operation, similarly to what we did in the <code>EmitTelemetryMessagesAsync</code>. </p><blockquote>Now in the <code>EmitTelemetryMessagesAsync</code>we can loop forever and break based on the cancellation token signal. </blockquote><p>Finally, we can register to this cancellation event and when this event fires, let the Main method return by awaiting the <code>WhenCancelled</code> function. In other words, now we have an event driven mechanism to gracefully exit from all active threads of our program.</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2020/04/termination.gif" class="kg-image" alt="Restarted by request"></figure><p>The rules of thumb here are:</p><ol><li>Pass a reference of the CancellationToken to every async function, and to every the long-running synchronous. Use this token signal to gracefully return from these functions.</li><li>Pass a reference to the CancellationTokenSource to every function that performs critical operations that cannot recover from possible exceptions, e.g. initialization of used dependencies. Use the source to cancel the execution of the program.</li><li>Let the main function return when a cancellation is signaled. Allow some time for the other threads to complete their shutdown process.</li></ol><p>In the <a href="https://havedatawilltrain.com/the-waiting-loop/">next post</a> we will examine more elegant ways to implement telemetry pumps without the usage of while loops.</p>]]></content:encoded></item><item><title><![CDATA[The Last Judgment]]></title><description><![CDATA[This is the first post of a mini-series that aims to address the development experience and production readiness challenges in the space of IoT Edge, primarily under the lens of Azure IoT Edge.]]></description><link>https://havedatawilltrain.com/the-last-judgment/</link><guid isPermaLink="false">5e83c71e0b72be03aba4a5dd</guid><category><![CDATA[Azure IoT Edge]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Fri, 03 Apr 2020 23:44:19 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2020/03/TheLastJudgment.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2020/03/TheLastJudgment.jpg" alt="The Last Judgment"><p>Word is you like to live on the Edge, stranger.</p><p>Sometimes I like it too. I wish I didn't, but I do. I do it because that's where most of the interesting things happen, and for a good reason: <strong>zero latency</strong>. I wasn't always like that. I used to struggle with my equipment, didn't know where to start from, I was intimidated, I felt completely overwhelmed and powerless by this "technology". It took me some time to build up my skills and find my comfort zone, but to get there, the ride was rough. </p><p>For the past two years I've been working on various IoT Edge technologies. For the most part of it, my work was around the <strong><a href="https://azure.microsoft.com/en-us/services/iot-edge/">Azure IoT Edge</a> </strong>technology stack. Through this work, I came up with some ground rules that I personally think are the <strong>minimum prerequisites </strong>for any IoT development environment, especially for developers who write hardware-specific code, and some <strong>design principles </strong>that over time I've found great value in.</p><p>In this mini-series posts, I'll try to demonstrate these tools and patterns in an incremental way, starting from a simple console app that will gradually evolve to a production-ready Azure IoT Edge application. </p><h2 id="azure-iot-edge-the-main-gist">Azure IoT Edge: the main gist</h2><p>All edge technologies' purpose is to help you achieve one thing: <strong>run your code on a remote (network) device</strong>. This boils down to two distinct capabilities:</p><ol><li><strong>Deploy </strong>and <strong>manage </strong>your code on a device.</li><li>Establish communication with the device.</li></ol><p>We just experienced the proliferation of containers and, as expected, containers are the main deployment mechanism of Azure IoT Edge too. In Azure IoT Edge, one can deploy an application by creating the <strong>application manifest</strong>: a file that holds the information of which containers compose your application, and how these containers should be instantiated.</p><h2 id="the-development-tools-landscape">The development tools landscape</h2><p>There are development tools that help you build your containers and compose this manifest. Unfortunately, these tools rely heavily on the solution file system structure to operate, and come with unintuitive commands that mostly confuse rather than help. These tools rely on a thick stack of dependencies, that makes them flaky and slow, and print cryptic error messages. Even if you manage to properly install them on your development machine and use them to successfully compose your first IoT Edge application, the next challenge is to figure out how to debug this application. In a constant output log searching, lengthy redeployments, and remote debugging tricks, the whole development experience is frustrating.</p><p>The problem lies in the containerized nature of the deployment mechanism, and in the fact that this deployment mechanism was never abstracted out from the development experience. In addition to that, the deployment manifest is not generated by user code, and as a consequence, no code validation checks can apply to it. This means for example that tools like compilers, linters etc. are useless. If you decide to rename a class, any decent IDE/editor is clever enough to propagate this rename effect to the entire solution. But because this technology-unique manifest is exposed to the user, the application often breaks. Furthermore, it's difficult to have environment solution variants (debug, release etc.), because the compiler directives do not apply inside the manifest, and the tooling approach is to have dedicated replicas per configuration, making a large size application maintenance very difficult.   </p><h2 id="an-incremental-approach">An incremental approach</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2020/03/giphy.gif" class="kg-image" alt="The Last Judgment"><figcaption>RPC happiness</figcaption></figure><p>In it's most fundamental form, an IoT Edge application performs two functions:</p><ol><li>Processes device sensor data and emits some <strong>telemetry.</strong></li><li>Perform some form of <strong>RPC </strong>(remote command invocation).</li></ol><p>The typical documentation example of Azure IoT Edge is a <strong>temperature sensor</strong>, that emits the device temperature (telemetry), and has a reset method (command). As of writing this article, the source code of this example is <a href="https://github.com/Azure/iotedge/blob/2554c01d618446ebd8bcc1266d428d027a3d27f1/edge-modules/SimulatedTemperatureSensor/src/Program.cs">here</a>.</p><p>A simple console app that demonstrates this simulated temperature example looks like this:</p><figure class="kg-card kg-code-card"><pre><code class="language-csharp">using System;
using System.Threading;

namespace Example1
{
    class Program
    {
        // This is the program entry point
        static void Main(string[] args)
        {
            // Register the Reset command callback 
            RegisterCommandCallback("Reset", OnReset);

            // A non-blocking telemetry emission invocation
            new Thread(EmitTelemetryMessages).Start();

            // Wait until the app unloads or gets cancelled
            Console.ReadLine();
        }

        private static void RegisterCommandCallback(string command,
            Action callback)
        {
            // Perform the command registration
            // Code omitted
            return;
        }

        // A method exposed for RPC
        static void OnReset()
        {
            // Perform a temperature sensor reset
            // Code omitted
            return;
        }

        // Emit 500 telemetry messages
        static void EmitTelemetryMessages()
        {
            for (int i = 1; i &lt;= 500; i++)
            {
                Console.WriteLine($"Sending telemetry message {i} ..");
                Thread.Sleep(1000);
            }
        }
    }
}
</code></pre><figcaption>A simple IoT Edge Temperature Sensor</figcaption></figure><p>Let's break down what happens here. This example code performs three things:</p><ol><li>Registers a command callback (the actual registration code is omitted for clarity)</li><li>Sends some messages in a second thread</li><li>Waits for the user to press enter to exit.</li></ol><p>The latter step is there to ensure that the program does not exit immediately, before the other thread has the chance to send any message. In practice though, in a typical IoT Edge scenario there is no user to press enter, there's not even a monitor screen to print these messages. We'll see later on what's the best pattern to deal with this issue.</p><p>As with most containerized applications, this is a command-line initiated app, a <strong>console app</strong>. Console apps come from a time when all applications were single threaded and there was no GUI to interact with. The operating system would load the application binary code into a memory block, and start a thread at the program entry point, the <strong>main</strong> function. When this function returned, the program would exit. </p><p>When multithreaded applications were eventually allowed by modern operating systems, programmers could manually create a new thread and start it at any function. This felt like running multiple applications, but sharing the same memory block. This capability allowed the creation of a new programming style, called <strong>multithreaded event-driven</strong> that was very useful in scenarios like computer networking or graphical user interfaces, or in general, when functions had to be executed in a non-predefined order and timing, but rather based on external events.</p><p>Fast forward a few decades, and the modern languages have made it easier to deal with multithreaded event driven programming, generally called asynchronous programming . Tons of literature has been written about this topic, but the fundamental idea is simple: writing code that can react to external events of undetermined timing and order. C# in particular has the <strong><a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/async">async</a>/<a href="https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/operators/await">await</a> pattern</strong>, which is an elegant way to execute some code in parallel (code that runs in a different thread). </p><p>Using this pattern, the above example becomes:</p><pre><code class="language-csharp">using System;
using System.Threading.Tasks;

namespace Example2
{
    class Program
    {
        // This is the program entry point
        static async Task Main(string[] args)
        {
            // Register the Reset command callback 
            await RegisterCommandCallbackAsync("Reset", OnReset);

            // A non-blocking telemetry emission invocation
            await EmitTelemetryMessagesAsync();

            // Wait until the app unloads or gets cancelled
            Console.ReadLine();
        }

        private static async Task RegisterCommandCallbackAsync(string command,
            Action callback)
        {
            // Perform the command registration
            // Code omitted 
            return;
        }

        // A method exposed for RPC
        static void OnReset()
        {
            // Perform a temperature sensor reset
            // Code omitted
            return;
        }

        // Emit 500 telemetry messages
        static async Task EmitTelemetryMessagesAsync()
        {
            for (int i = 1; i &lt;= 500; i++)
            {
                Console.WriteLine($"Sending telemetry message {i} ..");
                await Task.Delay(TimeSpan.FromSeconds(1));
            }
        }
    }
}</code></pre><blockquote>Note that we had to refactor our main function signature to be async</blockquote><p>These two versions are very similar in functionality, but with a major difference: in the latter example, there is no code segment that runs for an extensive period, in other words, <u>no thread is blocked by waiting for something to complete</u>. Indeed, in the first example, the thread we created to run the <code>EmitTelemetryMessages</code> function would periodically wait, blocked for a second at the <code>Thread.Sleep(1000)</code>. Blocked in this context means that the thread, although running, is waiting for something else to continue. This is not too important for now, but in real production systems becomes very important and helps to avoid the <u><a href="https://docs.microsoft.com/en-us/dotnet/standard/threading/the-managed-thread-pool">thread pool </a>starvation issue</u>, aka, running out of threads. </p><p>The rule of thumb here is: <u>anything that might take a considerable amount of time to complete, has to be implemented in the async/await pattern</u>. The important word here is <strong>might</strong>: generally speaking, things that require IO, like reading a file from the disk, sending a message over the network etc. fall into this category. Practically anything that does not run only in the context of the CPU should be awaited, because of the uncertainty of the completion time of the required additional hardware resource. But even for code that runs solely in the boundaries of the CPU, if it could take more than a few milliseconds to run, it should be awaited.</p><p>In the <a href="https://havedatawilltrain.com/restarted-by-request/">next article </a>we will examine the best approach to keep the main function from returning.</p>]]></content:encoded></item><item><title><![CDATA[Stream of the Jest]]></title><description><![CDATA[Exploring an optimized streaming Object Detection application pipeline using the DeepStream SDK and SSD Inception V2 COCO on a Jetson device.]]></description><link>https://havedatawilltrain.com/stream-of-the-jest/</link><guid isPermaLink="false">5d9cdad332f8e9003c17823e</guid><category><![CDATA[Jetson TX2]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[TensorRT]]></category><category><![CDATA[ARM]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Fri, 11 Oct 2019 21:29:44 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.jpg" alt="Stream of the Jest"><p>In a <a href="https://havedatawilltrain.com/three-threads-to-perdido/">previous post </a>we explored a high performance implementation of a Python <strong>Object Detector </strong>based on the <strong>SSD Inception V2 COCO </strong>model running on an <strong>NVIDIA Jetson TX2</strong>. In this post we will explore how we can implement the same Object Detector using NVIDIA's <strong><a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a>.</strong></p><p>Previously, in the process of increasing the observed detector Frames Per Second (FPS) we saw how we can optimize the model with TensorRT and at the same time replace the simple synchronous while-loop implementation to an asynchronous multi-threaded one.</p><p>We noticed the increased FPS and the introduced trade-offs: the increased inference latency and the increased CPU and GPU utilization. Reading the web camera input frame-by-frame and pushing the data to the GPU for inference can be very challenging.</p><h2 id="deepstream-object-detector-on-a-jetson-tx2">DeepStream Object Detector on a Jetson TX2</h2><p>The data generated by a web camera are streaming data. By definition, data streams are continuous sources of data, in other words, sources that you cannot pause in any way. One of the strategies in processing data streams is to record the data and run an offline processing pipeline. This is called batch processing.</p><p>In our case we are interested in minimizing the latency of inference. A more appropriate strategy then would be a real time stream processing pipeline.<strong> </strong>The <a href="https://developer.nvidia.com/deepstream-sdk"><strong>DeepStream SDK</strong></a><strong> </strong>offers different<strong> hardware accelerated</strong> <strong>plugins </strong>to compose different real time stream processing pipelines. </p><p>NVIDIA also offers <a href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream-l4t"><strong>pre-built containers</strong></a> for running a containerized DeepStream application. Unfortunately, at the time of this blog post, the containers had some missing dependencies. For this reason, this application is run directly on the TX2 device.</p><h3 id="deepstream-object-detector-application-setup">DeepStream Object Detector application setup</h3><p>To run a custom DeepSteam object detector pipeline with the SSD Inception V2 COCO model on a TX2, run the following commands. </p><blockquote>I'm using JetPack 4.2.1</blockquote><h3 id="step-1-get-the-model-"><strong>Step 1: Get the model. </strong></h3><p>This command will download and extract in <code>/temp</code> the same model we used <a href="https://havedatawilltrain.com/three-threads-to-perdido/">in the Python implementation.</a></p><pre><code class="language-bash">wget -qO- http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz | tar xvz -C /tmp</code></pre><h3 id="step-2-optimize-the-model-with-tensorrt">Step 2: Optimize the model with TensorRT</h3><p>This command will convert this downloaded frozen graph to a UFF MetaGraph model.</p><pre><code class="language-bash">python3 /usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py \
  /tmp/ssd_inception_v2_coco_2017_11_17/frozen_inference_graph.pb -O NMS \
  -p /usr/src/tensorrt/samples/sampleUffSSD/config.py \
  -o /tmp/sample_ssd_relu6.uff</code></pre><p>The generated UFF file is here: <code>/tmp/sample_ssd_relu6.uff</code>.</p><h3 id="step-3-compile-the-custom-object-detector-application">Step 3: Compile the custom object detector application</h3><p>This will build the <code>nvdsinfer_custom_impl</code> sample that comes with the SDK.</p><pre><code class="language-bash">cd /opt/nvidia/deepstream/deepstream-4.0/sources/objectDetector_SSD
CUDA_VER=10.0 make -C nvdsinfer_custom_impl</code></pre><p>We need to copy the UFF MetaGraph along with the label names file inside this folder.</p><pre><code class="language-bash">cp /usr/src/tensorrt/data/ssd/ssd_coco_labels.txt .
cp /tmp/sample_ssd_relu6.uff .</code></pre><h3 id="step-4-edit-the-application-configuration-file-to-use-your-camera">Step 4: Edit the application configuration file to use your camera</h3><p>Located in the same directory, the file <code>deepstream_app_config_ssd.txt</code> contains information about the DeepStream pipeline components, including the input source. The original example has been configured to use a static file as source.</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI
type=3
num-sources=1
uri=file://../../samples/streams/sample_1080p_h264.mp4
gpu-id=0
cudadec-memtype=0</code></pre><p>If you want to use a USB camera, make a copy of this file and change the above section to:</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI
type=1
camera-width=1280
camera-height=720
camera-fps-n=30
camera-fps-d=1
camera-v4l2-dev-node=1</code></pre><p>Save this as <code>deepstream_app_config_ssd_USB.txt</code>.</p><p>TX2 comes with an on board CSI camera. If you want to use the embedded CSI camera change the source to:</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP 5=CSI
type=5
camera-width=1280
camera-height=720
camera-fps-n=30
camera-fps-d=1</code></pre><p>Save this file as <code>deepstream_app_config_ssd_CSI.txt</code>.</p><blockquote>Both cameras are configured to run at <strong>30 FPS with 1280x720 </strong>resolution. </blockquote><h3 id="execute-the-application">Execute the application</h3><p>To execute the Object Detection application using the USB camera, run:</p><pre><code class="language-bash">deepstream-app -c deepstream_app_config_ssd_USB.txt
</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/USB.gif" class="kg-image" alt="Stream of the Jest"><figcaption>DeepStream detector @16 FPS with the USB Camera</figcaption></figure><p>The application will print the FPS in the terminal, it's <strong>~16 FPS. </strong>This result is very similar to the TensorRT Python <a href="https://havedatawilltrain.com/three-threads-to-perdido/">implementation</a> where we had achieved 15 FPS in a simple Python while loop. </p><p>So what's the fuzz about DeepStream?</p><p>In a closer look, we can tell there is a substantial difference. The <code>tegrastats</code> output shows an semi-utilized GPU (~50%) and an under utilized CPU (~25%).</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/image-3.png" class="kg-image" alt="Stream of the Jest"></figure><p>The USB camera throughput is the obvious bottleneck in this pipeline. The Jetson TX2 <a href="https://developer.nvidia.com/embedded/jetson-tx2-developer-kit">development kit </a>comes with an on board 5 MP Fixed Focus MIPI CSI Camera out of the box.</p><p>The Camera Serial Interface (CSI) is a specification of the Mobile Industry Processor Interface (MIPI) Alliance. It defines an interface between a camera and a host processor. This means that the CSI camera can move the data in the GPU faster than the USB port.</p><p>Let's try the CSI camera.</p><pre><code class="language-bash">deepstream-app -c deepstream_app_config_ssd_CSI.txt
</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/ezgif-1-17a272e47f88.gif" class="kg-image" alt="Stream of the Jest"><figcaption>DeepStream detector @24 FPS with the CSI Camera</figcaption></figure><p>The average performance now is <strong>~24 FPS. </strong>Note that the theoretical maximum we can get is 30 FPS, since this is the camera frame rate.</p><p>We can see an obvious increase of the system utilization:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/image-4.png" class="kg-image" alt="Stream of the Jest"></figure><p>Let's try running both applications side by side:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/both-1.gif" class="kg-image" alt="Stream of the Jest"><figcaption>~12 FPS for each application</figcaption></figure><p>The GPU now is operating at full capacity.</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/image-5.png" class="kg-image" alt="Stream of the Jest"></figure><p>The observed frame rates are <strong>~12 FPS </strong>for each application. Apparently, the <strong>maximum capacity of this GPU is ~24 inferences per second</strong> for this setup.</p><blockquote><em>Note: NVIDIA advertises <a href="https://developer.nvidia.com/deepstream-sdk">t</a>hat a DeepStream ResNet-based object<br>detector application can handle<strong> concurrently 14 independent 1080p 30fps </strong>video streams on a TX2. Here we see that <strong>this is far from true</strong> when using an industry standard detection model like SSD Inception V2 COCO.</em></blockquote><p><strong>What have we achieved: </strong><u>We've explored an optimized streaming Object Detection application pipeline using the DeepStream SDK and we've achieved the maximum detection throughput possible, as defined by the device's hardware limits.</u></p>]]></content:encoded></item><item><title><![CDATA[Three Threads to Perdido]]></title><description><![CDATA[This is the first part of a blog series that explores the topic of "High Throughput Object Detection" on the Edge.]]></description><link>https://havedatawilltrain.com/three-threads-to-perdido/</link><guid isPermaLink="false">5d321251435c041126c989a3</guid><category><![CDATA[TensorFlow]]></category><category><![CDATA[Object Detection]]></category><category><![CDATA[TensorRT]]></category><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Jetson TX2]]></category><category><![CDATA[Docker]]></category><category><![CDATA[ARM]]></category><category><![CDATA[Containers]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Fri, 04 Oct 2019 18:56:00 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2019/10/cowboys-2277376-2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2019/10/cowboys-2277376-2.jpg" alt="Three Threads to Perdido"><p>Welcome stranger, I've been expecting you.</p><p>I know what brought you here, it's despair. I know what is out there,  I've seen it, and it's not pretty. Bad designs, broken code samples, no container definitions, missing dependency libraries, poor performance, you name it. </p><p>I was recently exploring ways to do<strong> real time object detection </strong>on my <strong>Nvidia Jetson TX2. </strong>The <em>real time </em>term here simply means, low latency and high throughput. It's a very loosely defined term, but it's used here in contrast to the store-and-process pattern, where storage is used as an interim stage.</p><hr><h2 id="high-performance-objection-detection-on-a-jetson-tx2">High Performance Objection Detection on a Jetson TX2</h2><h3 id="starting-simple">Starting simple</h3><p>We'll explore a simple program that detects human faces using the camera input and renders the camera input with the bounding boxes. This one is based on the<strong> <a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html">Haar Cascades</a></strong><a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html"> </a>and is one of the simplest ways to get started with Object Detection on the Edge. There is no Jetson platform dependency for this code, only on <strong>OpenCV</strong>.</p><blockquote><em>I'm using a remote development setup to do all of my coding that uses containers. This way you can experiment with all the code samples yourself without having to setup any runtime dependencies on your device.</em><br><br><a href="https://havedatawilltrain.com/got-nano-will-code/"><strong>Here </strong></a>is how I setup my device and my <a href="https://havedatawilltrain.com/drama-of-entropy/"><strong>remote development environment</strong></a> with VSCode.</blockquote><p>Start by cloning the <a href="https://github.com/paloukari/jetson-detectors">example code</a>. After cloning, you need to build and run the container that we'll be using to run our code.</p><!--kg-card-begin: markdown--><h3 id="clonetheexamplerepo">Clone the example repo</h3>
<pre><code>https://github.com/paloukari/jetson-detectors
cd jetson-detectors
</code></pre>
<h3 id="tobuildandrunthedevelopmentcontainer">To build and run the development container</h3>
<pre><code>sudo docker build . -f ./docker/Dockerfile.cpu -t object-detection-cpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v &quot;$PWD&quot;:/src -p 32001:22 object-detection-cpu
</code></pre>
<!--kg-card-end: markdown--><p>The <code>--privileged</code> is required for accessing all the devices. Alternatively you can use the <code>--device /dev/video0</code>. </p><p><a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">Here's the code we'll be running.</a> Simple open the <code>cpudetector.py</code> file in VSCode and hit F5 or just run: <code>python3 src/cpudetector.py</code>. In both cases you'll need to setup the X forwarding. See the <a href="https://havedatawilltrain.com/got-nano-will-code/">Step 2: X forwarding</a> on how to do this.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/cpu-2.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~23 FPS with OpenCV</figcaption></figure><p><strong>We get about 23 FPS</strong>. Use the <code>tegrastats</code> to see what's happening in the GPU:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/image.png" class="kg-image" alt="Three Threads to Perdido"></figure><p>We're interested in the <code>GR3D_FREQ</code> values. It's clear that this code runs only on the device CPUs with more than 75% utilization per core, and with <strong>0% GPU utilization</strong>.</p><h3 id="next-up-we-use-go-deep">Next up, we use go Deep</h3><p>Haar Cascades is good, but how about detecting more things at once? In this case, we need to use Deep Neural Networks. We will need to use another container from now on to run the following code. </p><h3 id="to-build-and-run-the-gpu-accelerated-container">To build and run the GPU accelerated container</h3><pre><code>sudo docker build . -f ./docker/Dockerfile.gpu -t object-detection-gpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v "$PWD":/src -p 32001:22 object-detection-gpu
</code></pre><blockquote>WARNING: This build takes a few hours to complete on a TX2. The main reason is because we build the Protobuf library to increase to models loading performance. To reduce this the build time, you can <a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson#building-jetson-containers-on-an-x86-workstation-using-qemu">build the same container on a X64 workstation</a>.</blockquote><p>In the first attempt, we'll be using the official <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">TensorFlow pre-trained networks</a>. The code we'll be running is <a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">here</a>.</p><p>When you run <code>python3 src/gpudetector.py --model-name ssd_inception_v2_coco</code> , the code will try to download the specified model inside the <code>/models</code> folder, and start the object detection in a very similar fashion as we did before. The <code>--model-name</code> default value is <code>ssd_inception_v2_coco</code>, so you can omit it.</p><p>This model has been trained to detect 90 classes (you can see the details in the downloaded <code>pipeline.config</code> file). Our performance plummeted to <strong>~8 FPS.</strong></p><p>Run <code>python3 src/gpudetector.py</code></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/gpu.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~8 FPS with a the TensorFlow SSD Inception V2 COCO model</figcaption></figure><p>What happened? We've started running the inference in the GPU which for a single inference round trip now takes more time. Also, we move now a lot of data from the camera to RAM and from there to the GPU. This has an obvious performance penalty. </p><p>What we can do is start with optimizing the inference. We'll use the TensorRT optimization to speedup the inference. Run the same file as before, but now with the <code>--trt-optimize</code> flag. This flag will convert the specified TensorFlow mode to a TensorRT and save if to a local file for the next time.</p><p>Run <code>python3 gpudetector.py --trt-optimize</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/ezgif-5-2f354c50d6b6.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~15 FPS with TensorRT optimization&nbsp;</figcaption></figure><p>Better, but still far from perfect. The way we can tell is by looking at the GPU utilization in the background, it drops periodically to 0%. This happens because the code is being executed sequentially. In other words, for each frame we have to wait to get the bits from the camera, create an in memory copy, push it to the GPU, perform the inference, and render the original frame with the scoring results.</p><p>We can break down this sequential execution to an asynchronous parallel version. We can have a dedicated thread for reading the data from the camera, one for running inference in the GPU and one for rendering the results.</p><p>To test this version, run  <code>python3 gpudetectorasync.py --trt-optimize</code></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/10/gpurtasync.gif" class="kg-image" alt="Three Threads to Perdido"><figcaption>~40 FPS with async TensorRT</figcaption></figure><p>By parallelizing expensive calculations, we've achieved a better performance compared to the OpenCV first example. The trade off now is that we've introduced a slight delay between the current frame and the corresponding inference result. To be more precise here, because the inference now is running on an independent thread, the observed FPS do not match with the number of inferences per second.</p><p><strong>What we have achieved:</strong> <u>We've explored different ways of improving the performance of the typical pedagogic object detection while-loop.</u></p><p>Next,<a href="https://havedatawilltrain.com/stream-of-the-jest/"> in this post </a>we'll explore how we can improve even more our detector, by using the <a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a>.</p>]]></content:encoded></item><item><title><![CDATA[Drama of entropy]]></title><description><![CDATA[Remote development using Docker Containers on a Jetson Device]]></description><link>https://havedatawilltrain.com/drama-of-entropy/</link><guid isPermaLink="false">5d8aad58bea92c00581d20fc</guid><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Jetson TX2]]></category><category><![CDATA[Remote Debugging]]></category><category><![CDATA[VSCode]]></category><category><![CDATA[ARM]]></category><category><![CDATA[Docker]]></category><category><![CDATA[Containers]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Mon, 30 Sep 2019 20:17:07 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2019/10/Electrical-poles.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2019/10/Electrical-poles.jpg" alt="Drama of entropy"><p>I used to mess up my development environment over time by installing all the different experimentation dependencies. This is why I decided to switch to Docker containers.</p><p>The <strong>Jetson Nano </strong>default image comes with Docker runtime pre-installed. To package your app and avoid polluting your device OS with various dependencies, you can use Docker containers as development environments.</p><p>In fact, using a container as a development environment is a very similar to using directly a Jetson device for remote development. <a href="https://havedatawilltrain.com/got-nano-will-code/">Here's my post</a> on how to do the latter.</p><hr><h2 id="remote-debugging-inside-a-container-running-on-a-jetson-device">Remote Debugging inside a Container running on a Jetson Device</h2><p></p><p>The <strong><a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson">NVIDIA Container Runtime on Jetson</a> </strong>repo<strong> </strong>has great information on how to use GPU accelerated containers in both the Jetson family, and an X64 workstation.</p><p>Once you choose what's your base container, you'll need to setup a few more things. </p><p>You can either clone <strong><a href="https://github.com/paloukari/jetson-detectors">this repo</a></strong> that contains all the below code, or start from scratch. The assumed file structure of all the following code is:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/image-2.png" class="kg-image" alt="Drama of entropy"></figure><blockquote><strong>/keys: </strong>Here's where I keep my public ssh key.<br><strong>/.vscode:</strong> Here's where I keep my debugging configuration.<br><strong>/docker: </strong>Inside the <strong>docker </strong>folder I put the container definitions.<br><strong>/src: </strong>Here's the application source code.</blockquote><h3 id="-keys">/keys</h3><p>You will need to setup a passwordless ssh between your host's VSCode and the container. To do this, you'll need to copy your ssh public key in the container's <code>authorized_keys</code> file. To get your public ssh key, run:<code>cat ~/.ssh/id_rsa.pub</code>. Copy the key value in the <code>id_rsa.pub</code> key file that's inside the /keys folder.</p><blockquote>My host is a Windows 10 machine and I generally make sure that my Windows and WSL keys are the same. To do this, I've copied my keys from <code>~/.ssh/</code> to <code>/mnt/c/users/[YOUR USERNAME]/.ssh</code>/. This way I get a convenience security symmetry.</blockquote><h3 id="-vscode">/.vscode</h3><p>Here's the my <code>launch.json</code> and how to setup an X Server for forwarding any GUI running on the container to your host (<a href="https://havedatawilltrain.com/got-nano-will-code/">here's</a> more information how to set this up)</p><!--kg-card-begin: markdown--><pre><code>{    
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;console&quot;: &quot;integratedTerminal&quot;, 
            &quot;env&quot;:
            {
            &quot;DISPLAY&quot;: &quot;10.135.62.79:0.0&quot; 
            }
        }
    ]
}
</code></pre>
<!--kg-card-end: markdown--><blockquote>Replace the IP with your host's IP. I'm using the <strong>X410 X Server</strong> on Windows 10.</blockquote><h3 id="-docker">/docker</h3><p>In this example, I've started from a bare minimum, the<strong> l4t OS</strong>. This dockerfile is self explanatory. </p><!--kg-card-begin: markdown--><pre><code>FROM nvcr.io/nvidia/l4t-base:r32.2

ENV DEBIAN_FRONTEND=noninteractive

# Install Python3, Git and OpenCV
RUN apt-get update &amp;&amp; apt-get --yes install openssh-server python3-dev python3-pip python3-opencv git
RUN pip3 install --upgrade pip

RUN pip3 install click

ENV LC_ALL C.UTF-8
ENV LANG C.UTF-8

# Set the WORKDIR
WORKDIR /src

ENTRYPOINT service ssh restart &amp;&amp; bash

# Install the ssh public key - Remove this in a production deployment
COPY ./keys/id_rsa.pub /tmp/tmp.pub
RUN mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh &amp;&amp; cat /tmp/tmp.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 ~/.ssh/authorized_keys &amp;&amp; rm -f /tmp/tmp.pub

</code></pre>
<!--kg-card-end: markdown--><h3 id="-src">/src</h3><p>This is where I put all the application code. You can test that the X forwarding works by opening running the <code>xtest.py</code>. You'll have to install the <code>eog</code> application in the container first by running <code>apt-get install eog</code> in a new VSCode terminal.</p><p>Here's the <code>xtest.py</code> code:</p><!--kg-card-begin: markdown--><pre><code>import subprocess
subprocess.run([&quot;eog&quot;])
</code></pre>
<!--kg-card-end: markdown--><h2 id="wiring-everything-up">Wiring everything up</h2><h3 id="on-your-jetson-device-run-">On your Jetson device, run:</h3><!--kg-card-begin: markdown--><pre><code># Clone the example repo
git clone https://github.com/paloukari/jetson-detectors
cd jetson-remote-development

# Build the dev container
sudo docker build . -f ./docker/Dockerfile.cpu -t object-detection-cpu

# Run the container
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v &quot;$PWD&quot;:/src -p 32001:22 object-detection-cpu
</code></pre>
<!--kg-card-end: markdown--><p>In your host's VSCode, add this information in the Remote-SSH configuration file:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/09/image-6.png" class="kg-image" alt="Drama of entropy"></figure><!--kg-card-begin: markdown--><pre><code>Host Nano
    User spyros
    HostName spyros-nano2
    IdentityFile ~/.ssh/id_rsa

Host NanoContainer
    User root
    HostName spyros-nano2
    IdentityFile ~/.ssh/id_rsa
    Port 32001

</code></pre>
<!--kg-card-end: markdown--><p>Now, you should be able to connect to the NanoContainer Remote SSH host. Last step is to install the Python VSCode extension on the remote host. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/09/image-8.png" class="kg-image" alt="Drama of entropy"><figcaption>The VSCode extensions need to be installed in both machines in a Remote-SSH session</figcaption></figure><p>Open the <code>xtest.py</code> and fit F5. </p><blockquote>You need to install the eog app in the container.</blockquote><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/10/ezgif-5-44d36401c343.gif" class="kg-image" alt="Drama of entropy"></figure><p><strong>What we achieved: </strong><u>We can now do remote development from a Windows host to a container running on a Jetson Nano, using VSCode.</u></p>]]></content:encoded></item><item><title><![CDATA[Got Nano, Wanna Code]]></title><description><![CDATA[Learn how to setup a remote development environment using VSCode from a Windows machine on a ARM device]]></description><link>https://havedatawilltrain.com/got-nano-will-code/</link><guid isPermaLink="false">5d7bf22d3694ea005ad2a5f8</guid><category><![CDATA[Jetson Nano]]></category><category><![CDATA[Remote Debugging]]></category><category><![CDATA[VSCode]]></category><category><![CDATA[ARM]]></category><dc:creator><![CDATA[Spyros Garyfallos]]></dc:creator><pubDate>Wed, 25 Sep 2019 20:39:41 GMT</pubDate><media:content url="https://havedatawilltrain.com/content/images/2019/09/Jetson-Nano_3QTR-Front_Left_trimmed-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://havedatawilltrain.com/content/images/2019/09/Jetson-Nano_3QTR-Front_Left_trimmed-1.jpg" alt="Got Nano, Wanna Code"><p>So, you just got your Jetson Nano and you're wondering how to get started? In this blog post you'll learn how to setup a remote development environment from your Windows 10 machine to a Jetson device.</p><h2 id="device-setup">Device Setup</h2><p>First, let's start with the device initial setup. To prepare my device, I followed the official <a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#intro">getting started guide </a>from Nvidia. I used a <a href="https://www.amazon.com/SanDisk-128GB-Extreme-microSD-Adapter/dp/B07FCMKK5X/ref=asc_df_B07FCMKK5X/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=309776868400&amp;hvpos=1o1&amp;hvnetw=g&amp;hvrand=8725959158767136122&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1027744&amp;hvtargid=pla-588455240877&amp;psc=1">128 GB microSD</a> and a <a href="https://www.amazon.com/5V-4000mA-switching-power-supply/dp/B01LY5TG5Y">power adapter</a> to power up my device, along with the <a href="https://www.jetsonhacks.com/2019/04/10/jetson-nano-use-more-power/">required jumper</a>. I connected a Dell P2415Q monitor using an HDMI cable and my USB receiver for my Logitech Triathlon keyboard and mouse. My development laptop is a Windows 10 machine.</p><h2 id="desktop-sharing">Desktop Sharing </h2><p>The desktop sharing app is broken. Follow <a href="https://blog.hackster.io/getting-started-with-the-nvidia-jetson-nano-developer-kit-43aa7c298797">this</a> article to fix it. You can also find instructions there on how to use Microsoft's Remote Desktop.</p><h2 id="development-environment">Development Environment</h2><p>The latest trend of development experience in the software development industry is developing using <strong>CLIs </strong>and <strong>code editors</strong>. I generally enjoy having all moving parts together when I'm coding, or at least in proximity.</p><p><strong>Visual Studio Code</strong> succeeded in combining together a cross-platform code editor, community driven plugins and decent UI/UX for development. But what I really like with Visual Studio Code is the recent <strong><a href="https://code.visualstudio.com/docs/remote/remote-overview">Remote Development</a> </strong>capability that allows you to keep a local UI/UX experience, while working on a remote host or container.</p><p>The other thing I like having in my development environment is <strong>idempotency</strong>: not having to deal with the dependencies of the host. <strong>Containers </strong>allow me to have multiple dependency configurations on the same host, and moreover, to share code between different hosts. For this reason, in every code sample I present in this blog, I make sure to include the corresponding container.</p><h3 id="step-1-remote-development-on-a-jetson-nano">Step 1: Remote Development on a Jetson Nano</h3><p>Because the ARM64 architecture is not officially supported yet, to install<strong> Visual Studio Code</strong> on your <strong>Jetson Nano</strong>, for now you'll have to use the <a href="https://code.headmelted.com/">community binaries</a>.</p><p><strong>Installing Visual Studio Code on a Jetson Nano</strong></p><pre><code class="language-Bash"># Start an elevated session
sudo -s

# Install the community repo key
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0CC3FD642696BFC8

# Run the installation script
. &lt;( wget -O - https://code.headmelted.com/installers/apt.sh )
</code></pre><p>If all goes well, you should get this output:</p><pre><code class="language-Bash">Installation complete!

You can start code at any time by calling "code-oss" within a terminal.

A shortcut should also now be available in your desktop menus (depending on your distribution).</code></pre><p>Next, you'll need to install the <strong><a href="https://code.visualstudio.com/insiders/">Visual Studio Code Insiders</a> on your host </strong>and install the <strong><a href="https://code.visualstudio.com/docs/remote/remote-overview">Remote Development</a> </strong>extension.</p><p>As a final step, you'll need to setup a passwordless SSH between your host and the Nano. To do this, you'll need to create an SSH public-private key pair and configure your device to trust your public key.</p><p>To setup your local SSH key in Windows run:</p><pre><code class="language-Powershell">Add-WindowsCapability -Online -Name OpenSSH.Client~~~~0.0.1.0
ssh-keygen -t rsa -b 4096 
</code></pre><p>The keys will be created here: %USERPROFILE%\.ssh</p><p>To copy your public key to the device:</p><pre><code class="language-CMD">SET REMOTEHOST=user@device
scp %USERPROFILE%\.ssh\id_rsa.pub %REMOTEHOST%:~/tmp.pub
ssh %REMOTEHOST% "mkdir -p ~/.ssh &amp;&amp; chmod 700 ~/.ssh &amp;&amp; cat ~/tmp.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 ~/.ssh/authorized_keys &amp;&amp; rm -f ~/tmp.pub"</code></pre><!--kg-card-begin: markdown--><blockquote>
<p>You'll need to replace the value <strong>user@device</strong></p>
</blockquote>
<!--kg-card-end: markdown--><p>In you host VSCode, hit F1 and type <em>Remote-SSH: Open Configuration File..</em></p><p>Add your configuration:</p><pre><code class="language-Remote-SSH config">Host Nano
    User user
    HostName device
    IdentityFile ~/.ssh/id_rsa</code></pre><!--kg-card-begin: markdown--><blockquote>
<p>Replace the values <strong>user</strong> and <strong>device</strong></p>
</blockquote>
<!--kg-card-end: markdown--><p>Connect to your Nano clicking the low left corner green icon:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/09/image-1.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><p>Congrats! Once connected, you can open a remote folder and open remote terminals directly in VSCode.</p><h3 id="step-2-x-forwarding">Step 2: X forwarding</h3><p>What about applications that have a GUI? No worries, you can setup an X forwarding from Nano to your host. To do this, I'm using the <a href="https://token2shell.com/x410/"><strong>X410 server</strong></a> for Windows. After installing and running the server, make sure you allow public access.</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/09/image-2.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><p>All is left to do is to configure the X Forwarding on your Nano device.</p><p>Open a terminal in VSCode (<strong>Ctrl+Shift+`</strong> on Windows) and run:</p><pre><code class="language-Remote-SSH config">export DISPLAY=10.135.62.79:0.0</code></pre><!--kg-card-begin: markdown--><blockquote>
<p>Make sure you replace the above IP with your host IP</p>
</blockquote>
<!--kg-card-end: markdown--><p> To verify everything works, from the terminal run:</p><pre><code class="language-Remote-SSH config">eog</code></pre><p>If all goes well, you should see this window:</p><figure class="kg-card kg-image-card"><img src="https://havedatawilltrain.com/content/images/2019/09/image-3.png" class="kg-image" alt="Got Nano, Wanna Code"></figure><p>To automatically set the remote variable when debugging your app, you can modify your<strong> launch.json </strong>and set the variable there:</p><pre><code class="language-launch.json">{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal", 
            "env":
            {
            "DISPLAY": "10.135.62.79:0.0" 
            }
        }
    ]
}</code></pre><p>To test all this, create a new python file containing the following code:</p><pre><code class="language-test.py">import subprocess
subprocess.run(["eog"])</code></pre><p>Now you can hit <strong>F5 </strong>and you'll get the same development experience, as with your local host, but<strong> running on a remote ARM machine!</strong></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://havedatawilltrain.com/content/images/2019/09/ezgif-5-44d36401c343.gif" class="kg-image" alt="Got Nano, Wanna Code"><figcaption>Starting a remote debugging session on Jetson Nano</figcaption></figure><p><strong>What we have achieved: </strong><u>We can now do remote development from a Windows host to a Jetson Nano, using VSCode.</u></p><p>Next, we'll setup the same remote environment, <a href="https://havedatawilltrain.com/drama-of-entropy/">but on a Docker container running on the Jetson device</a>.</p>]]></content:encoded></item></channel></rss>