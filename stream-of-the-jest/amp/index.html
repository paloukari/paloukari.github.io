<!DOCTYPE html>
<html âš¡>
<head>
    <meta charset="utf-8">

    <title>Stream of the Jest</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="/favicon.png" type="image/png" />
    <link rel="canonical" href="https://havedatawilltrain.com/stream-of-the-jest/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="HAVE DATA - WILL TRAIN" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Stream of the Jest" />
    <meta property="og:description" content="Exploring an optimized streaming Object Detection application pipeline using the DeepStream SDK and SSD Inception V2 COCO on a Jetson device." />
    <meta property="og:url" content="https://havedatawilltrain.com/stream-of-the-jest/" />
    <meta property="og:image" content="https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.jpg" />
    <meta property="article:published_time" content="2019-10-11T21:29:44.000Z" />
    <meta property="article:modified_time" content="2020-04-06T16:33:32.000Z" />
    <meta property="article:tag" content="Jetson TX2" />
    <meta property="article:tag" content="Object Detection" />
    <meta property="article:tag" content="TensorRT" />
    <meta property="article:tag" content="ARM" />
    
    <meta property="article:publisher" content="https://www.facebook.com/spyros.garyfallos" />
    <meta property="article:author" content="https://www.facebook.com/spyros.garyfallos" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Stream of the Jest" />
    <meta name="twitter:description" content="Exploring an optimized streaming Object Detection application pipeline using the DeepStream SDK and SSD Inception V2 COCO on a Jetson device." />
    <meta name="twitter:url" content="https://havedatawilltrain.com/stream-of-the-jest/" />
    <meta name="twitter:image" content="https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Spyros Garyfallos" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Jetson TX2, Object Detection, TensorRT, ARM" />
    <meta name="twitter:site" content="@SpyrosCarnation" />
    <meta name="twitter:creator" content="@SpyrosCarnation" />
    <meta property="og:image:width" content="750" />
    <meta property="og:image:height" content="495" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "HAVE DATA - WILL TRAIN",
        "url": "https://havedatawilltrain.com/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://havedatawilltrain.com/content/images/2019/07/Logo2.png"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Spyros Garyfallos",
        "image": {
            "@type": "ImageObject",
            "url": "https://havedatawilltrain.com/content/images/2019/09/Capture.PNG",
            "width": 375,
            "height": 439
        },
        "url": "https://havedatawilltrain.com/author/spyros/",
        "sameAs": [
            "https://www.facebook.com/spyros.garyfallos",
            "https://twitter.com/SpyrosCarnation"
        ]
    },
    "headline": "Stream of the Jest",
    "url": "https://havedatawilltrain.com/stream-of-the-jest/",
    "datePublished": "2019-10-11T21:29:44.000Z",
    "dateModified": "2020-04-06T16:33:32.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.jpg",
        "width": 750,
        "height": 495
    },
    "keywords": "Jetson TX2, Object Detection, TensorRT, ARM",
    "description": "Exploring an optimized streaming Object Detection application pipeline using the DeepStream SDK and SSD Inception V2 COCO on a Jetson device.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://havedatawilltrain.com/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.12" />
    <link rel="alternate" type="application/rss+xml" title="HAVE DATA - WILL TRAIN" href="https://havedatawilltrain.com/rss/" />

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,600,400" />
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-anim" src="https://cdn.ampproject.org/v0/amp-anim-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="https://havedatawilltrain.com">HAVE DATA - WILL TRAIN</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Stream of the Jest</h1>
                <section class="post-meta">
                    <p class="author">by <a href="/author/spyros/">Spyros Garyfallos</a></p>
                    <time class="post-date" datetime="2019-10-11">2019-10-11</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://havedatawilltrain.com/content/images/2019/10/053747ada3375e440325c09210983c7c.jpg" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>In a <a href="https://havedatawilltrain.com/three-threads-to-perdido/">previous post </a>we explored a high performance implementation of a Python <strong>Object Detector </strong>based on the <strong>SSD Inception V2 COCO </strong>model running on an <strong>NVIDIA Jetson TX2</strong>. In this post we will explore how we can implement the same Object Detector using NVIDIA's <strong><a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a>.</strong></p><p>Previously, in the process of increasing the observed detector Frames Per Second (FPS) we saw how we can optimize the model with TensorRT and at the same time replace the simple synchronous while-loop implementation to an asynchronous multi-threaded one.</p><p>We noticed the increased FPS and the introduced trade-offs: the increased inference latency and the increased CPU and GPU utilization. Reading the web camera input frame-by-frame and pushing the data to the GPU for inference can be very challenging.</p><h2 id="deepstream-object-detector-on-a-jetson-tx2">DeepStream Object Detector on a Jetson TX2</h2><p>The data generated by a web camera are streaming data. By definition, data streams are continuous sources of data, in other words, sources that you cannot pause in any way. One of the strategies in processing data streams is to record the data and run an offline processing pipeline. This is called batch processing.</p><p>In our case we are interested in minimizing the latency of inference. A more appropriate strategy then would be a real time stream processing pipeline.<strong> </strong>The <a href="https://developer.nvidia.com/deepstream-sdk"><strong>DeepStream SDK</strong></a><strong> </strong>offers different<strong> hardware accelerated</strong> <strong>plugins </strong>to compose different real time stream processing pipelines. </p><p>NVIDIA also offers <a href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream-l4t"><strong>pre-built containers</strong></a> for running a containerized DeepStream application. Unfortunately, at the time of this blog post, the containers had some missing dependencies. For this reason, this application is run directly on the TX2 device.</p><h3 id="deepstream-object-detector-application-setup">DeepStream Object Detector application setup</h3><p>To run a custom DeepSteam object detector pipeline with the SSD Inception V2 COCO model on a TX2, run the following commands. </p><blockquote>I'm using JetPack 4.2.1</blockquote><h3 id="step-1-get-the-model-"><strong>Step 1: Get the model. </strong></h3><p>This command will download and extract in <code>/temp</code> the same model we used <a href="https://havedatawilltrain.com/three-threads-to-perdido/">in the Python implementation.</a></p><pre><code class="language-bash">wget -qO- http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz | tar xvz -C /tmp</code></pre><h3 id="step-2-optimize-the-model-with-tensorrt">Step 2: Optimize the model with TensorRT</h3><p>This command will convert this downloaded frozen graph to a UFF MetaGraph model.</p><pre><code class="language-bash">python3 /usr/lib/python3.6/dist-packages/uff/bin/convert_to_uff.py \
  /tmp/ssd_inception_v2_coco_2017_11_17/frozen_inference_graph.pb -O NMS \
  -p /usr/src/tensorrt/samples/sampleUffSSD/config.py \
  -o /tmp/sample_ssd_relu6.uff</code></pre><p>The generated UFF file is here: <code>/tmp/sample_ssd_relu6.uff</code>.</p><h3 id="step-3-compile-the-custom-object-detector-application">Step 3: Compile the custom object detector application</h3><p>This will build the <code>nvdsinfer_custom_impl</code> sample that comes with the SDK.</p><pre><code class="language-bash">cd /opt/nvidia/deepstream/deepstream-4.0/sources/objectDetector_SSD
CUDA_VER=10.0 make -C nvdsinfer_custom_impl</code></pre><p>We need to copy the UFF MetaGraph along with the label names file inside this folder.</p><pre><code class="language-bash">cp /usr/src/tensorrt/data/ssd/ssd_coco_labels.txt .
cp /tmp/sample_ssd_relu6.uff .</code></pre><h3 id="step-4-edit-the-application-configuration-file-to-use-your-camera">Step 4: Edit the application configuration file to use your camera</h3><p>Located in the same directory, the file <code>deepstream_app_config_ssd.txt</code> contains information about the DeepStream pipeline components, including the input source. The original example has been configured to use a static file as source.</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI
type=3
num-sources=1
uri=file://../../samples/streams/sample_1080p_h264.mp4
gpu-id=0
cudadec-memtype=0</code></pre><p>If you want to use a USB camera, make a copy of this file and change the above section to:</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI
type=1
camera-width=1280
camera-height=720
camera-fps-n=30
camera-fps-d=1
camera-v4l2-dev-node=1</code></pre><p>Save this as <code>deepstream_app_config_ssd_USB.txt</code>.</p><p>TX2 comes with an on board CSI camera. If you want to use the embedded CSI camera change the source to:</p><pre><code>[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP 5=CSI
type=5
camera-width=1280
camera-height=720
camera-fps-n=30
camera-fps-d=1</code></pre><p>Save this file as <code>deepstream_app_config_ssd_CSI.txt</code>.</p><blockquote>Both cameras are configured to run at <strong>30 FPS with 1280x720 </strong>resolution. </blockquote><h3 id="execute-the-application">Execute the application</h3><p>To execute the Object Detection application using the USB camera, run:</p><pre><code class="language-bash">deepstream-app -c deepstream_app_config_ssd_USB.txt
</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>DeepStream detector @16 FPS with the USB Camera</figcaption></figure><p>The application will print the FPS in the terminal, it's <strong>~16 FPS. </strong>This result is very similar to the TensorRT Python <a href="https://havedatawilltrain.com/three-threads-to-perdido/">implementation</a> where we had achieved 15 FPS in a simple Python while loop. </p><p>So what's the fuzz about DeepStream?</p><p>In a closer look, we can tell there is a substantial difference. The <code>tegrastats</code> output shows an semi-utilized GPU (~50%) and an under utilized CPU (~25%).</p><figure class="kg-card kg-image-card"></figure><p>The USB camera throughput is the obvious bottleneck in this pipeline. The Jetson TX2 <a href="https://developer.nvidia.com/embedded/jetson-tx2-developer-kit">development kit </a>comes with an on board 5 MP Fixed Focus MIPI CSI Camera out of the box.</p><p>The Camera Serial Interface (CSI) is a specification of the Mobile Industry Processor Interface (MIPI) Alliance. It defines an interface between a camera and a host processor. This means that the CSI camera can move the data in the GPU faster than the USB port.</p><p>Let's try the CSI camera.</p><pre><code class="language-bash">deepstream-app -c deepstream_app_config_ssd_CSI.txt
</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>DeepStream detector @24 FPS with the CSI Camera</figcaption></figure><p>The average performance now is <strong>~24 FPS. </strong>Note that the theoretical maximum we can get is 30 FPS, since this is the camera frame rate.</p><p>We can see an obvious increase of the system utilization:</p><figure class="kg-card kg-image-card"></figure><p>Let's try running both applications side by side:</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><figcaption>~12 FPS for each application</figcaption></figure><p>The GPU now is operating at full capacity.</p><figure class="kg-card kg-image-card"></figure><p>The observed frame rates are <strong>~12 FPS </strong>for each application. Apparently, the <strong>maximum capacity of this GPU is ~24 inferences per second</strong> for this setup.</p><blockquote><em>Note: NVIDIA advertises <a href="https://developer.nvidia.com/deepstream-sdk">t</a>hat a DeepStream ResNet-based object<br />detector application can handle<strong> concurrently 14 independent 1080p 30fps </strong>video streams on a TX2. Here we see that <strong>this is far from true</strong> when using an industry standard detection model like SSD Inception V2 COCO.</em></blockquote><p><strong>What have we achieved: </strong><u>We've explored an optimized streaming Object Detection application pipeline using the DeepStream SDK and we've achieved the maximum detection throughput possible, as defined by the device's hardware limits.</u></p>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="https://havedatawilltrain.com">HAVE DATA - WILL TRAIN</a> &copy; 2020</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
</html>
