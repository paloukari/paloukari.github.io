<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">

    <title>Three Threads to Perdido</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="/favicon.png" type="image/png" />
    <link rel="canonical" href="https://havedatawilltrain.com/three-threads-to-perdido/" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="HAVE DATA - WILL TRAIN" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Three Threads to Perdido" />
    <meta property="og:description" content="This is the first part of a blog series that explores the topic of &quot;High Throughput Object Detection&quot; on the Edge." />
    <meta property="og:url" content="https://havedatawilltrain.com/three-threads-to-perdido/" />
    <meta property="og:image" content="https://havedatawilltrain.com/content/images/2019/10/cowboys_twitter-2.png" />
    <meta property="article:published_time" content="2019-10-04T18:56:00.000Z" />
    <meta property="article:modified_time" content="2019-10-10T18:26:15.000Z" />
    <meta property="article:tag" content="TensorFlow" />
    <meta property="article:tag" content="Object Detection" />
    <meta property="article:tag" content="TensorRT" />
    <meta property="article:tag" content="Jetson Nano" />
    <meta property="article:tag" content="Jetson TX2" />
    <meta property="article:tag" content="Docker" />
    <meta property="article:tag" content="ARM" />
    <meta property="article:tag" content="Containers" />
    
    <meta property="article:publisher" content="https://www.facebook.com/spyros.garyfallos" />
    <meta property="article:author" content="https://www.facebook.com/spyros.garyfallos" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Three Threads to Perdido" />
    <meta name="twitter:description" content="This is the first part of a blog series that explores the topic of &quot;High Throughput Object Detection&quot; on the Edge." />
    <meta name="twitter:url" content="https://havedatawilltrain.com/three-threads-to-perdido/" />
    <meta name="twitter:image" content="https://havedatawilltrain.com/content/images/2019/10/cowboys_twitter-1.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Spyros Garyfallos" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="TensorFlow, Object Detection, TensorRT, Jetson Nano, Jetson TX2, Docker, ARM, Containers" />
    <meta name="twitter:site" content="@SpyrosCarnation" />
    <meta name="twitter:creator" content="@SpyrosCarnation" />
    <meta property="og:image:width" content="510" />
    <meta property="og:image:height" content="261" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "HAVE DATA - WILL TRAIN",
        "logo": "https://havedatawilltrain.com/content/images/2019/07/Logo2.png"
    },
    "author": {
        "@type": "Person",
        "name": "Spyros Garyfallos",
        "image": {
            "@type": "ImageObject",
            "url": "https://havedatawilltrain.com/content/images/2019/09/Capture.PNG",
            "width": 375,
            "height": 439
        },
        "url": "https://havedatawilltrain.com/author/spyros/",
        "sameAs": [
            "https://www.facebook.com/spyros.garyfallos",
            "https://twitter.com/SpyrosCarnation"
        ]
    },
    "headline": "Three Threads to Perdido",
    "url": "https://havedatawilltrain.com/three-threads-to-perdido/",
    "datePublished": "2019-10-04T18:56:00.000Z",
    "dateModified": "2019-10-10T18:26:15.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://havedatawilltrain.com/content/images/2019/10/cowboys-2277376-2.jpg",
        "width": 2000,
        "height": 2217
    },
    "keywords": "TensorFlow, Object Detection, TensorRT, Jetson Nano, Jetson TX2, Docker, ARM, Containers",
    "description": "This is the first part of a blog series that explores the topic of &quot;High Throughput Object Detection&quot; on the Edge.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://havedatawilltrain.com/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.31" />
    <link rel="alternate" type="application/rss+xml" title="HAVE DATA - WILL TRAIN" href="https://havedatawilltrain.com/rss/" />

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,600,400" />
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-anim" src="https://cdn.ampproject.org/v0/amp-anim-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="https://havedatawilltrain.com">HAVE DATA - WILL TRAIN</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Three Threads to Perdido</h1>
                <section class="post-meta">
                    <p class="author">by <a href="/author/spyros/">Spyros Garyfallos</a></p>
                    <time class="post-date" datetime="2019-10-04">2019-10-04</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://havedatawilltrain.com/content/images/2019/10/cowboys-2277376-2.jpg" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>Welcome stranger, I've been expecting you.</p><p>I know what brought you here, it's despair. I know what is out there,  I've seen it, and it's not pretty. Bad designs, broken code samples, no container definitions, missing dependency libraries, poor performance, you name it. </p><p>I was recently exploring ways to do<strong> real time object detection </strong>on my <strong>Nvidia Jetson TX2. </strong>The <em>real time </em>term here simply means, low latency and high throughput. It's a very loosely defined term, but it's used here in contrast to the store-and-process pattern, where storage is used as an interim stage.</p><hr></hr><h2 id="high-performance-objection-detection-on-a-jetson-tx2">High Performance Objection Detection on a Jetson TX2</h2><h3 id="starting-simple">Starting simple</h3><p>We'll explore a simple program that detects human faces using the camera input and renders the camera input with the bounding boxes. This one is based on the<strong> <a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html">Haar Cascades</a></strong><a href="https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html"> </a>and is one of the simplest ways to get started with Object Detection on the Edge. There is no Jetson platform dependency for this code, only on <strong>OpenCV</strong>.</p><blockquote><em>I'm using a remote development setup to do all of my coding that uses containers. This way you can experiment with all the code samples yourself without having to setup any runtime dependencies on your device.</em><br /><br /><a href="https://havedatawilltrain.com/got-nano-will-code/"><strong>Here </strong></a>is how I setup my device and my <a href="https://havedatawilltrain.com/drama-of-entropy/"><strong>remote development environment</strong></a> with VSCode.</blockquote><p>Start by cloning the <a href="https://github.com/paloukari/jetson-detectors">example code</a>. After cloning, you need to build and run the container that we'll be using to run our code.</p><h3 id="clonetheexamplerepo">Clone the example repo</h3>
<pre><code>https://github.com/paloukari/jetson-detectors
cd jetson-detectors
</code></pre>
<h3 id="tobuildandrunthedevelopmentcontainer">To build and run the development container</h3>
<pre><code>sudo docker build . -f ./docker/Dockerfile.cpu -t object-detection-cpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v "$PWD":/src -p 32001:22 object-detection-cpu
</code></pre>
<p>The <code>--privileged</code> is required for accessing all the devices. Alternatively you can use the <code>--device /dev/video0</code>. </p><p><a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">Here's the code we'll be running.</a> Simple open the <code>cpudetector.py</code> file in VSCode and hit F5 or just run: <code>python3 src/cpudetector.py</code>. In both cases you'll need to setup the X forwarding. See the <a href="https://havedatawilltrain.com/got-nano-will-code/">Step 2: X forwarding</a> on how to do this.</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>~23 FPS with OpenCV</figcaption></figure><p><strong>We get about 23 FPS</strong>. Use the <code>tegrastats</code> to see what's happening in the GPU:</p><figure class="kg-card kg-image-card"></figure><p>We're interested in the <code>GR3D_FREQ</code> values. It's clear that this code runs only on the device CPUs with more than 75% utilization per core, and with <strong>0% GPU utilization</strong>.</p><h3 id="next-up-we-use-go-deep">Next up, we use go Deep</h3><p>Haar Cascades is good, but how about detecting more things at once? In this case, we need to use Deep Neural Networks. We will need to use another container from now on to run the following code. </p><h3 id="to-build-and-run-the-gpu-accelerated-container">To build and run the GPU accelerated container</h3><pre><code>sudo docker build . -f ./docker/Dockerfile.gpu -t object-detection-gpu
sudo docker run --rm --runtime nvidia --privileged -ti -e DISPLAY=$DISPLAY -v "$PWD":/src -p 32001:22 object-detection-gpu
</code></pre><blockquote>WARNING: This build takes a few hours to complete on a TX2. The main reason is because we build the Protobuf library to increase to models loading performance. To reduce this the build time, you can <a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson#building-jetson-containers-on-an-x86-workstation-using-qemu">build the same container on a X64 workstation</a>.</blockquote><p>In the first attempt, we'll be using the official <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">TensorFlow pre-trained networks</a>. The code we'll be running is <a href="https://github.com/paloukari/jetson-detectors/blob/master/src/cpudetector.py">here</a>.</p><p>When you run <code>python3 src/gpudetector.py --model-name ssd_inception_v2_coco</code> , the code will try to download the specified model inside the <code>/models</code> folder, and start the object detection in a very similar fashion as we did before. The <code>--model-name</code> default value is <code>ssd_inception_v2_coco</code>, so you can omit it.</p><p>This model has been trained to detect 90 classes (you can see the details in the downloaded <code>pipeline.config</code> file). Our performance plummeted to <strong>~8 FPS.</strong></p><p>Run <code>python3 src/gpudetector.py</code></p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>~8 FPS with a the TensorFlow SSD Inception V2 COCO model</figcaption></figure><p>What happened? We've started running the inference in the GPU which for a single inference round trip now takes more time. Also, we move now a lot of data from the camera to RAM and from there to the GPU. This has an obvious performance penalty. </p><p>What we can do is start with optimizing the inference. We'll use the TensorRT optimization to speedup the inference. Run the same file as before, but now with the <code>--trt-optimize</code> flag. This flag will convert the specified TensorFlow mode to a TensorRT and save if to a local file for the next time.</p><p>Run <code>python3 gpudetector.py --trt-optimize</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>~15 FPS with TensorRT optimization </figcaption></figure><p>Better, but still far from perfect. The way we can tell is by looking at the GPU utilization in the background, it drops periodically to 0%. This happens because the code is being executed sequentially. In other words, for each frame we have to wait to get the bits from the camera, create an in memory copy, push it to the GPU, perform the inference, and render the original frame with the scoring results.</p><p>We can break down this sequential execution to an asynchronous parallel version. We can have a dedicated thread for reading the data from the camera, one for running inference in the GPU and one for rendering the results.</p><p>To test this version, run  <code>python3 gpudetectorasync.py --trt-optimize</code></p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>~40 FPS with async TensorRT</figcaption></figure><p>By parallelizing expensive calculations, we've achieved a better performance compared to the OpenCV first example. The trade off now is that we've introduced a slight delay between the current frame and the corresponding inference result. To be more precise here, because the inference now is running on an independent thread, the observed FPS do not match with the number of inferences per second.</p><p><strong>What we have achieved:</strong> <u>We've explored different ways of improving the performance of the typical pedagogic object detection while-loop.</u></p><p>In a future post we'll explore how we can improve even more our detector, by using the <a href="https://developer.nvidia.com/deepstream-sdk">DeepStream SDK</a>.</p>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="https://havedatawilltrain.com">HAVE DATA - WILL TRAIN</a> &copy; 2019</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
</html>
